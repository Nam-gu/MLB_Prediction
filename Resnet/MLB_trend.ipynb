{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLB_trend.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnrhwoN/Li6kf0FLnJ2tZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nam-gu/MLB_Prediction/blob/main/Resnet/MLB_trend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js3eOSFET_RR",
        "outputId": "b697433a-ea08-4f97-f33a-9fa35864dad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import lightgbm\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTOlyyJSUhhN",
        "outputId": "eb127480-db8d-4de6-b981-f40039dd3273"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/MLP_Prediction/MLB_Prediction/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7kRn_05UqUg",
        "outputId": "51cb2bd6-de7a-49af-e0bd-56e4967d1413"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MLP_Prediction/MLB_Prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = './data/Retrosheet_2010_2019/2010_to_2019_seasons.csv'\n",
        "game_df = pd.read_csv(file)\n",
        "\n",
        "game_col_del = '''VisitorGDP,VisitorCI,HomeGDP,HomeCI, DoubleHeader, DayOfWeek, VisitingTeamLeague, HomeTeamLeague, DayNight, CompletionInfo,ForfeitInfo, ProtestInfo,Attendence, Duration, VisitorLineScore, HomeLineScore, UmpireHID, UmpireHName, Umpire1BID, Umpire1BName, Umpire2BID,Umpire2BName, Umpire3BID, Umpire3BName, UmpireLFID, UmpireLFName, UmpireRFID,UmpireRFName, VisitorManagerID, VisitorManagerName, HomeManagerID, HomeManagerName,WinningPitcherID, WinningPitcherName, LosingPitcherID, LosingPitcherNAme,SavingPitcherID, SavingPitcherName, GameWinningRBIID, GameWinningRBIName, VisitorBatting1Position, VisitorBatting2Position, VisitorBatting3Position,VisitorBatting4Position, VisitorBatting5Position, VisitorBatting6Position,VisitorBatting7Position, VisitorBatting8Position, VisitorBatting9Position,HomeBatting1Position, HomeBatting2Position, HomeBatting3Position,HomeBatting4Position, HomeBatting5Position, HomeBatting6Position,HomeBatting7Position, HomeBatting8Position, HomeBatting9Position,AdditionalInfo, AcquisitionInfo'''\n",
        "game_col_del = game_col_del.replace(\" \", \"\")\n",
        "game_col_del = game_col_del.split(\",\")\n",
        "\n",
        "game_df.drop(game_col_del,axis=1,inplace=True)\n",
        "\n",
        "\n",
        "col_del = ['VisitorBatting1PlayerID', 'VisitorBatting1Name', 'VisitorBatting2PlayerID', 'VisitorBatting2Name', 'VisitorBatting3PlayerID', 'VisitorBatting3Name', 'VisitorBatting4PlayerID', 'VisitorBatting4Name', 'VisitorBatting5PlayerID', 'VisitorBatting5Name', 'VisitorBatting6PlayerID', 'VisitorBatting6Name', 'VisitorBatting7PlayerID', 'VisitorBatting7Name', 'VisitorBatting8PlayerID', 'VisitorBatting8Name', 'VisitorBatting9PlayerID', 'VisitorBatting9Name', 'HomeBatting1PlayerID', 'HomeBatting1Name', 'HomeBatting2PlayerID', 'HomeBatting2Name', 'HomeBatting3PlayerID', 'HomeBatting3Name', 'HomeBatting4PlayerID', 'HomeBatting4Name', 'HomeBatting5PlayerID', 'HomeBatting5Name', 'HomeBatting6PlayerID', 'HomeBatting6Name', 'HomeBatting7PlayerID', 'HomeBatting7Name', 'HomeBatting8PlayerID', 'HomeBatting8Name', 'HomeBatting9PlayerID', 'HomeBatting9Name']\n",
        "game_df.drop(col_del,axis=1,inplace=True)\n",
        "game_df = game_df.rename(columns={\"VisitingTeam\":\"VisitorTeam\",\"VisitingTeamGameNumber\":\"VisitorTeamGameNumber\"})"
      ],
      "metadata": {
        "id": "fPW_xsKhUrLR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "away_inning = game_df['LengthInOuts']//2\n",
        "home_inning = game_df['LengthInOuts']- away_inning\n",
        "\n",
        "\n",
        "game_df['VisitorOffInn'] = away_inning\n",
        "game_df['VisitorDifInn'] = home_inning\n",
        "game_df['HomeOffInn'] = home_inning\n",
        "game_df['HomeDifInn'] = away_inning\n",
        "\n",
        "game_df['Date'] = pd.to_datetime(game_df['Date'].astype(str), format='%Y%m%d')\n",
        "game_df['current_year'] = game_df['Date'].dt.year\n",
        "game_df['Home_team_won?'] = game_df['HomeRunsScore'] > game_df['VisitorRunsScored']\n",
        "game_df['Visitor_team_won?'] = game_df['HomeRunsScore'] < game_df['VisitorRunsScored']"
      ],
      "metadata": {
        "id": "RAsJFcXlUwXN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bat_stat = [ 'TeamGameNumber','Team','AB','H','D','T','HR','RBI','SH','SF',\n",
        "'HBP','BB','IBB','K','SB','CS','LOB','Pitchers',\n",
        "'ER','TER','WP','Balks','PO','A','E','Passed','DB','TP','OffInn','DifInn','_team_won?']\n",
        "pit_stat = ['H', 'D', 'T', 'HR', 'RBI', 'SH', 'SF', 'HBP', 'BB', 'IBB', 'K', 'LOB', 'ER','DB','AB']"
      ],
      "metadata": {
        "id": "P6br71nBU2wT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_stat = ['Date']\n",
        "select_stat += ['Home'+i for i in bat_stat]+['Visitor'+i for i in pit_stat]\n",
        "rename1 = {'Visitor'+i:'Pit'+i for i in pit_stat}\n",
        "rename2 = {'Home'+i:i for i in bat_stat}"
      ],
      "metadata": {
        "id": "aQNfVyFCU4Zj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Home 시각 \n",
        "select_stat = ['Date']\n",
        "select_stat += ['Home'+i for i in bat_stat]+['Visitor'+i for i in pit_stat]\n",
        "rename1 = {'Visitor'+i:'Pit'+i for i in pit_stat}\n",
        "rename2 = {'Home'+i:i for i in bat_stat}\n",
        "Home = game_df.copy()\n",
        "Home = Home[select_stat].rename(columns=rename1)\n",
        "Home = Home.rename(columns=rename2)\n",
        "# away 시각\n",
        "select_stat = ['Date']\n",
        "select_stat += ['Visitor'+i for i in bat_stat]+['Home'+i for i in pit_stat]\n",
        "rename1 = {'Home'+i:'Pit'+i for i in pit_stat}\n",
        "rename2 = {'Visitor'+i:i for i in bat_stat}\n",
        "Visitor = game_df.copy()\n",
        "Visitor = Visitor[select_stat].rename(columns=rename1)\n",
        "Visitor = Visitor.rename(columns=rename2)"
      ],
      "metadata": {
        "id": "-_AyWprDU5De"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sep_team = pd.concat([Home,Visitor])\n",
        "sep_team = sep_team.rename(columns={'PitDB':'DP'})"
      ],
      "metadata": {
        "id": "vaJ_O3-bU6F9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['AB', 'H', 'D', 'T', 'HR', 'RBI','DP',\n",
        "       'SF', 'BB', 'K', 'SB', 'CS', 'LOB', 'Pitchers', 'ER', 'TER', 'WP','PitAB',\n",
        "       'Balks', 'PO', 'A', 'E', 'Passed', 'DB', 'TP', 'OffInn', 'DifInn',\n",
        "       'PitH', 'PitD', 'PitT', 'PitHR', 'PitRBI', 'PitSF',\n",
        "       'PitBB', 'PitK', 'PitLOB', 'PitER']"
      ],
      "metadata": {
        "id": "tVqMpentU8ir"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trend = sep_team.copy()\n",
        "sep_team['year']= sep_team.Date.dt.year\n",
        "sep_team[col] = sep_team.groupby(['year','Team'])[col].transform(lambda x: x.expanding(1).sum())\n",
        "sep_team[col] = sep_team[col].subtract(trend[col])\n",
        "trend['year']= trend.Date.dt.year\n",
        "trend[col] = trend.groupby(['year','Team'])[col].transform(lambda x: x.rolling(window=5,closed='left').sum())\n",
        "sep_team = sep_team[sep_team['TeamGameNumber']>=10]\n",
        "trend = trend[trend['TeamGameNumber']>=10]"
      ],
      "metadata": {
        "id": "vxVGMW2XU-7c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_feature = ['Date', 'TeamGameNumber', 'Team','_team_won?']\n",
        "# feature selection\n",
        "select_feature += ['AB', 'H', 'D', 'T', 'HR', 'RBI','BB', 'K','DP', 'HBP','IBB','SF','SH',#bat stat\n",
        "                    'ER','OffInn','DifInn' # game info\n",
        "                    ,'PitAB','PitH', 'PitD', 'PitT', 'PitHR', 'PitRBI', 'PitSF', 'PitBB',\"PitIBB\",\"PitHBP\" ,'PitK','PitLOB', 'PitER'\n",
        "                    ]\n"
      ],
      "metadata": {
        "id": "-4CKCMDQVAOb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sep = sep_team[select_feature] "
      ],
      "metadata": {
        "id": "eCbhX9IiVOhZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H, OBP, SLG, ISO, Babip, RC, wOBA X, K, BB, RBI, K/B, HR   \n",
        "# 타자 가공\n",
        "sep['RC'] = ((sep['H']+sep['BB']+sep['HBP']-sep['DP'])\n",
        "        *(sep['H']+2*sep['D']+3*sep['T']+4*sep['HR']+0.52*(sep['SF']+sep['SH'])+0.26*(sep['BB']+sep['HBP']-sep['IBB']))\n",
        "        )/(sep['AB']*sep['AB']) #득점 생산\n",
        "\n",
        "\n",
        "#사구,사사구, 고의사구 통합\n",
        "sep['BB'] += sep['HBP']+sep['IBB']\n",
        "sep['PitBB'] += sep['PitHBP']+sep['PitIBB']\n",
        "#del sep['HBP'],sep['IBB'],sep['PitHBP'],sep['PitIBB']\n",
        "# 희생번트, 희생플라이 통합\n",
        "sep['PitSF'] += sep_team['PitSH']\n",
        "sep['SF'] += sep['SH']\n",
        "#del sep['SH'],sep['PitSH']\n",
        "\n",
        "sep['K/B'] = sep['K']/(sep['BB']+0.5)\n",
        "sep['PA'] = sep['AB'] - sep['BB'] - sep['SF'] # 타수\n",
        "sep['Babip'] = (sep['H']+sep['D']+sep['T']) / (sep['PA']-sep['K']-sep['HR']-sep['SF']) # 인플레이 타구\n",
        "sep['SLG'] = (sep['H']+2*sep['D']+3*sep['T']+4*sep['HR'])/sep['PA'] # 장타율\n",
        "sep['H'] = sep['H']/sep['PA'] # 안타율\n",
        "sep['D'] = sep['D']/sep['PA'] # 안타율\n",
        "sep['T'] = sep['T']/sep['PA'] # 안타율\n",
        "sep['HR'] = sep['HR']/sep['PA'] # 안타율\n",
        "sep['BB'] = sep['BB']/sep['PA'] # 볼넷\n",
        "sep['K'] = sep['K']/sep['PA']\n",
        "sep['RBI'] = sep['RBI']/sep['TeamGameNumber'] # 득점\n",
        "\n",
        "\n",
        "sep['OBP'] = (sep['H']+sep['BB']) #출루율\n",
        "sep['OPS'] = sep['SLG']+sep['OBP'] #OPS\n",
        "sep['GPA'] = (1.8*sep['OBP']+sep['SLG'])/4#GPA(Gross Production Average) - park factor를 적용해야함\n",
        "sep['ISO'] = sep['SLG'] - sep['H']\n"
      ],
      "metadata": {
        "id": "I_cgvEYaVRZ2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 투수 가공\n",
        "# PitH, PitOBP, PitSLG, PitOPS, PitHR, EAR, PitB, PitK, PitK/B, WHIP, kwERA\n",
        "sep['ERA'] = sep['PitER']/sep['DifInn']\n",
        "sep['TotalH'] = sep['PitH']+sep['PitD']+sep['PitT']+sep['PitHR']\n",
        "sep['TotalBB'] = sep['PitBB']+sep['PitHBP']\n",
        "sep['PitPA'] = sep['PitAB'] - sep['TotalBB'] - sep['PitSF']\n",
        "sep['kwERA'] = (5.40-12*(sep['PitK'] - sep['TotalBB']))/sep['PitPA']\n",
        "sep['WHIP'] = (sep['TotalH']+sep['TotalBB'])/sep['DifInn']\n",
        "sep['PitK'] = sep['PitK']/sep['DifInn']\n",
        "sep['PitB'] = sep['TotalBB']/sep['DifInn']\n",
        "sep['PitK/B'] = sep['PitK']/(sep['PitB']+0.5)\n",
        "\n",
        "\n",
        "sep['PitSLG'] = (sep['PitH']+2*sep['PitD']+3*sep['PitT']+4*sep['PitHR'])/sep['PitPA'] # 피장타율\n",
        "sep['PitH'] = sep['PitH']/sep['PitPA'] # 피안타율\n",
        "sep['PitOBP'] = (sep['TotalH']+sep['TotalBB']) / sep['PitAB'] #피출루율\n",
        "sep['PitOPS'] = sep['PitSLG']+sep['PitOBP'] #피OPS\n",
        "sep['PitHR'] /= sep['PitPA']"
      ],
      "metadata": {
        "id": "iKcLPPfeVSEy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#temp = sep[['Date', 'TeamGameNumber', 'Team', '_team_won?','GPA', 'OPS','OBP','H','SLG','Babip','RC']]\n",
        "temp = sep\n",
        "game_log = game_df[['Date','VisitorTeam','HomeTeam','VisitorTeamGameNumber','HomeTeamGameNumber']]\n",
        "game_log = pd.merge(game_log,temp,left_on = ['Date','VisitorTeam'], right_on = ['Date','Team'],how='left')\n",
        "game_log = pd.merge(game_log,temp,left_on = ['Date','HomeTeam'], right_on = ['Date','Team'],how='left')"
      ],
      "metadata": {
        "id": "80qjICiNVTM4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_feature1 = ['Date', 'TeamGameNumber', 'Team','_team_won?']\n",
        "select_feature2 = ['AB', 'H', 'D', 'T', 'HR', 'RBI','BB', 'K','DP', 'HBP','IBB','SF','SH',#bat stat\n",
        "                    'ER','OffInn','DifInn' # game info\n",
        "                    ,'PitAB','PitH', 'PitD', 'PitT', 'PitHR', 'PitRBI', 'PitSF', 'PitBB',\"PitIBB\",\"PitHBP\" ,'PitK','PitLOB', 'PitER'\n",
        "                    ]\n",
        "sep = trend[select_feature1 + select_feature2] "
      ],
      "metadata": {
        "id": "JE1epvyeXNXU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H, OBP, SLG, ISO, Babip, RC, wOBA X, K, BB, RBI, K/B, HR   \n",
        "# 타자 가공\n",
        "sep['RC'] = ((sep['H']+sep['BB']+sep['HBP']-sep['DP'])\n",
        "        *(sep['H']+2*sep['D']+3*sep['T']+4*sep['HR']+0.52*(sep['SF']+sep['SH'])+0.26*(sep['BB']+sep['HBP']-sep['IBB']))\n",
        "        )/(sep['AB']*sep['AB']) #득점 생산\n",
        "\n",
        "\n",
        "#사구,사사구, 고의사구 통합\n",
        "sep['BB'] += sep['HBP']+sep['IBB']\n",
        "sep['PitBB'] += sep['PitHBP']+sep['PitIBB']\n",
        "#del sep['HBP'],sep['IBB'],sep['PitHBP'],sep['PitIBB']\n",
        "# 희생번트, 희생플라이 통합\n",
        "sep['PitSF'] += sep_team['PitSH']\n",
        "sep['SF'] += sep['SH']\n",
        "#del sep['SH'],sep['PitSH']\n",
        "\n",
        "sep['K/B'] = sep['K']/(sep['BB']+0.5)\n",
        "sep['PA'] = sep['AB'] - sep['BB'] - sep['SF'] # 타수\n",
        "sep['Babip'] = (sep['H']+sep['D']+sep['T']) / (sep['PA']-sep['K']-sep['HR']-sep['SF']) # 인플레이 타구\n",
        "sep['SLG'] = (sep['H']+2*sep['D']+3*sep['T']+4*sep['HR'])/sep['PA'] # 장타율\n",
        "sep['H'] = sep['H']/sep['PA'] # 안타율\n",
        "sep['D'] = sep['D']/sep['PA'] # 안타율\n",
        "sep['T'] = sep['T']/sep['PA'] # 안타율\n",
        "sep['HR'] = sep['HR']/sep['PA'] # 안타율\n",
        "sep['BB'] = sep['BB']/sep['PA'] # 볼넷\n",
        "sep['RBI'] = sep['RBI']/5 # 득점\n",
        "sep['K'] = sep['K']/sep['PA']\n",
        "\n",
        "sep['OBP'] = (sep['H']+sep['BB']) #출루율\n",
        "sep['OPS'] = sep['SLG']+sep['OBP'] #OPS\n",
        "sep['GPA'] = (1.8*sep['OBP']+sep['SLG'])/4#GPA(Gross Production Average) - park factor를 적용해야함\n",
        "sep['ISO'] = sep['SLG'] - sep['H']\n"
      ],
      "metadata": {
        "id": "5ECXCmZxXTni"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 투수 가공\n",
        "# PitH, PitOBP, PitSLG, PitOPS, PitHR, EAR, PitB, PitK, PitK/B, WHIP, kwERA\n",
        "sep['ERA'] = sep['PitER']/sep['DifInn']\n",
        "sep['TotalH'] = sep['PitH']+sep['PitD']+sep['PitT']+sep['PitHR']\n",
        "sep['TotalBB'] = sep['PitBB']+sep['PitHBP']\n",
        "sep['PitPA'] = sep['PitAB'] - sep['TotalBB'] - sep['PitSF']\n",
        "sep['kwERA'] = (5.40-12*(sep['PitK'] - sep['TotalBB']))/sep['PitPA']\n",
        "sep['WHIP'] = (sep['TotalH']+sep['TotalBB'])/sep['DifInn']\n",
        "sep['PitK'] = sep['PitK']/sep['DifInn']\n",
        "sep['PitB'] = sep['TotalBB']/sep['DifInn']\n",
        "sep['PitK/B'] = sep['PitK']/(sep['PitB']+0.5)\n",
        "\n",
        "\n",
        "sep['PitSLG'] = (sep['PitH']+2*sep['PitD']+3*sep['PitT']+4*sep['PitHR'])/sep['PitPA'] # 피장타율\n",
        "sep['PitH'] = sep['PitH']/sep['PitPA'] # 피안타율\n",
        "sep['PitOBP'] = (sep['TotalH']+sep['TotalBB']) / sep['PitAB'] #피출루율\n",
        "sep['PitOPS'] = sep['PitSLG']+sep['PitOBP'] #피OPS\n",
        "sep['PitHR'] /= sep['PitPA']"
      ],
      "metadata": {
        "id": "a6lVBh_5XTbR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#temp = sep[['Date', 'TeamGameNumber', 'Team', '_team_won?','GPA', 'OPS','OBP','H','SLG','Babip','RC']]\n",
        "select = ['H','OBP','SLG','ISO','Babip','RC','K','BB','RBI','K/B','HR','PitH','PitOBP','PitSLG','PitOPS','PitHR','ERA','PitB','PitK','PitK/B','WHIP']\n",
        "temp = sep[select_feature1 + select]\n",
        "\n",
        "game_log = game_log[game_log.VisitorTeamGameNumber>=10]\n",
        "game_log = game_log[game_log.HomeTeamGameNumber>=10]\n",
        "\n",
        "trend_columns = select_feature1 + [\"trend\" + i for i in select]\n",
        "temp.columns = trend_columns\n",
        "temp.drop(['TeamGameNumber', '_team_won?'],axis=1, inplace=True)\n",
        "game_log = pd.merge(game_log,temp,left_on = ['Date','VisitorTeam'], right_on = ['Date','Team'],how='left')\n",
        "game_log = pd.merge(game_log,temp,left_on = ['Date','HomeTeam'], right_on = ['Date','Team'],how='left')\n",
        "\n"
      ],
      "metadata": {
        "id": "i1O63jBVX9St"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'H','OBP','SLG','ISO','Babip','RC','K','BB','RBI','K/B','HR','PitH','PitOBP','PitSLG','PitOPS','PitHR','EAR','PitB','PitK','PitK/B','WHIP'"
      ],
      "metadata": {
        "id": "572TUJyUVgiH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Home/visit\n",
        "y_cols = ['_team_won?_x']\n",
        "select = ['H','OBP','SLG','ISO','Babip','RC','K','BB','RBI','K/B','HR','PitH','PitOBP','PitSLG','PitOPS','PitHR','ERA','PitB','PitK','PitK/B','WHIP']\n",
        "select += [\"trend\" + i for i in select]\n",
        "game_log = game_log.dropna()\n",
        "for i in select:\n",
        "  game_log[i]=game_log[i+'_x']/(game_log[i+'_y'] + 1e+06)\n",
        "game_log['year'] = game_log.Date.dt.year\n",
        "train = game_log[game_log.year!=2019]\n",
        "test = game_log[game_log.year==2019]\n",
        "train = train.dropna()\n",
        "test = test.dropna()"
      ],
      "metadata": {
        "id": "oqf8e-LYVVul"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Team/Trend\n",
        "y_cols = ['_team_won?_x']\n",
        "select = ['H','OBP','SLG','ISO','Babip','RC','K','BB','RBI','K/B','HR','PitH','PitOBP','PitSLG','PitOPS','PitHR','ERA','PitB','PitK','PitK/B','WHIP']\n",
        "team = [i+'_x' for i in select]\n",
        "team += [i+'_y' for i in select]\n",
        "game_log = game_log.dropna()\n",
        "for i in team:\n",
        "  game_log[i]=game_log[i]/(game_log['trend'+i] + 1e+06)\n",
        "game_log['year'] = game_log.Date.dt.year\n",
        "train = game_log[game_log.year!=2019]\n",
        "test = game_log[game_log.year==2019]\n",
        "train = train.dropna()\n",
        "test = test.dropna()"
      ],
      "metadata": {
        "id": "dmGrhv55ysd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train[y_cols]*1\n",
        "y_test = test[y_cols]*1\n",
        "X_train = train[select]\n",
        "X_test = test[select]"
      ],
      "metadata": {
        "id": "HwvzxyWUVjI7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odv3jh5NsIaf",
        "outputId": "4522cd3d-a18f-4147-b84e-8f0deda1b5a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "H              3.468371e-07\n",
              "OBP            4.593447e-07\n",
              "SLG            7.079149e-07\n",
              "ISO            3.866212e-07\n",
              "Babip          5.992758e-07\n",
              "RC             2.486192e-07\n",
              "K              3.431972e-07\n",
              "BB             1.514397e-07\n",
              "RBI            4.609985e-05\n",
              "K/B            3.928081e-06\n",
              "HR             6.161500e-08\n",
              "PitH           3.375588e-07\n",
              "PitOBP         5.088650e-07\n",
              "PitSLG         6.769282e-07\n",
              "PitOPS         1.182975e-06\n",
              "PitHR          5.771286e-08\n",
              "ERA            2.242370e-07\n",
              "PitB           1.575910e-07\n",
              "PitK           4.159826e-07\n",
              "PitK/B         6.937188e-07\n",
              "WHIP           6.959851e-07\n",
              "trendH         4.725273e-07\n",
              "trendOBP       6.599998e-07\n",
              "trendSLG       1.010869e-06\n",
              "trendISO       6.408838e-07\n",
              "trendBabip     8.676466e-07\n",
              "trendRC        4.181250e-07\n",
              "trendK         4.696969e-07\n",
              "trendBB        3.023256e-07\n",
              "trendRBI       1.059996e-05\n",
              "trendK/B       2.999996e-05\n",
              "trendHR        1.160221e-07\n",
              "trendPitH      4.621211e-07\n",
              "trendPitOBP    7.111109e-07\n",
              "trendPitSLG    1.097014e-06\n",
              "trendPitOPS    1.802895e-06\n",
              "trendPitHR     1.132075e-07\n",
              "trendERA       3.925925e-07\n",
              "trendPitB      2.803030e-07\n",
              "trendPitK      5.259258e-07\n",
              "trendPitK/B    9.281041e-07\n",
              "trendWHIP      9.846150e-07\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train = (train[y_cols]*1).to_numpy()\n",
        "\n",
        "y_test = (test[y_cols]*1).to_numpy()\n",
        "\n",
        "model = sm.Logit(y_train,X_train)\n",
        "results = model.fit()\n",
        "print(results.summary())\n",
        "\n",
        "y_hat = results.predict(X_train)\n",
        "y_hat = list(map(round,y_hat))\n",
        "print('Train accuracy = ', accuracy_score(y_train, y_hat))\n",
        "\n",
        "y_hat = results.predict( X_test)\n",
        "y_hat = list(map(round,y_hat))\n",
        "print('Test accuracy = ', accuracy_score(y_test, y_hat))"
      ],
      "metadata": {
        "id": "VgeKKW6CctqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rtdl\n",
        "!pip install libzero==0.0.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP-CHFWFtxGc",
        "outputId": "0d8c4aa9-a19c-4a17-c09e-6a90df3b46fa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rtdl\n",
            "  Downloading rtdl-0.0.13-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.21.6)\n",
            "Requirement already satisfied: torch<2,>=1.7 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.7->rtdl) (4.2.0)\n",
            "Installing collected packages: rtdl\n",
            "Successfully installed rtdl-0.0.13\n",
            "Collecting libzero==0.0.4\n",
            "  Downloading libzero-0.0.4-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5,>=4.0 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (4.64.0)\n",
            "Requirement already satisfied: torch<2,>=1.6 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.11.0+cu113)\n",
            "Collecting pynvml<9,>=8.0\n",
            "  Downloading pynvml-8.0.4-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (4.2.0)\n",
            "Installing collected packages: pynvml, libzero\n",
            "Successfully installed libzero-0.0.4 pynvml-8.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict\n",
        "\n",
        "import numpy as np\n",
        "import rtdl\n",
        "import scipy.special\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import zero\n",
        "# device = torch.device('cpu')\n",
        "device = torch.device(\"cuda:0\")\n",
        "zero.improve_reproducibility(seed=123456)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-sQNmIt0VH",
        "outputId": "9b95cf69-79f2-46f3-9d0d-07c3f1678865"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123456"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = {}\n",
        "y = {}\n",
        "\n",
        "y_train = sklearn.preprocessing.LabelEncoder().fit_transform(y_train).astype('int64')\n",
        "y_test = sklearn.preprocessing.LabelEncoder().fit_transform(y_test).astype('int64')\n",
        "X['train']=X_train\n",
        "X['test']=X_test\n",
        "y['train']=y_train\n",
        "y['test']=y_test\n",
        "\n",
        "preprocess = sklearn.preprocessing.RobustScaler().fit(X['train'])\n",
        "X = {\n",
        "    k: torch.tensor(preprocess.fit_transform(v), device=device)\n",
        "    for k, v in X.items()\n",
        "}\n",
        "y = {k: torch.tensor(v, device=device) for k, v in y.items()}\n",
        "\n",
        "X['train']=torch.tensor(X['train'],dtype=torch.float32,device=device)\n",
        "X['test']=torch.tensor(X['test'],dtype=torch.float32, device=device)\n",
        "# X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(X['train'], y['train'], train_size=0.8)"
      ],
      "metadata": {
        "id": "U3iMws5Kt-Gy"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_type ='binclass'\n",
        "d_out = 1\n",
        "hyperparameters = {'d_min':32, 'd_hidden':128, 'n_blocks':2, 'Learning_rate':0.01,'weight_decay':0.01}\n",
        "model = rtdl.ResNet.make_baseline(\n",
        "    d_in=X_train.shape[1],\n",
        "    d_main=hyperparameters['d_min'],\n",
        "    d_hidden=hyperparameters['d_hidden'],\n",
        "    dropout_first=0.2,\n",
        "    dropout_second=0.0,\n",
        "    n_blocks=hyperparameters['n_blocks'],\n",
        "    d_out=d_out,\n",
        ")\n",
        "lr = hyperparameters['Learning_rate']\n",
        "weight_decay = hyperparameters['weight_decay']\n",
        "\n",
        "model.to(device)\n",
        "optimizer = (\n",
        "    model.make_default_optimizer()\n",
        "    if isinstance(model, rtdl.FTTransformer)\n",
        "    # else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else torch.optim.SGD(model.parameters(), lr=lr)\n",
        ")\n",
        "\n",
        "loss_fn = (\n",
        "    F.binary_cross_entropy_with_logits\n",
        "    if task_type == 'binclass'\n",
        "    else F.cross_entropy\n",
        "    if task_type == 'multiclass'\n",
        "    else F.mse_loss\n",
        ")"
      ],
      "metadata": {
        "id": "P2BUI4SIvsn5"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_model(x_num, x_cat=None):\n",
        "    if isinstance(model, rtdl.FTTransformer):\n",
        "        return model(x_num, x_cat)\n",
        "    elif isinstance(model, (rtdl.MLP, rtdl.ResNet)):\n",
        "        assert x_cat is None\n",
        "        return model(x_num)\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f'Looks like you are using a custom model: {type(model)}.'\n",
        "            ' Then you have to implement this branch first.'\n",
        "        )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(part):\n",
        "    model.eval()\n",
        "    prediction = []\n",
        "    for batch in zero.iter_batches(X[part], 1024):\n",
        "        prediction.append(apply_model(batch))\n",
        "    prediction = torch.cat(prediction).squeeze(1).cpu().numpy()\n",
        "    target = y[part].cpu().numpy()\n",
        "\n",
        "    if task_type == 'binclass':\n",
        "        prediction = np.round(scipy.special.expit(prediction))\n",
        "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
        "    elif task_type == 'multiclass':\n",
        "        prediction = prediction.argmax(1)\n",
        "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
        "    else:\n",
        "        assert task_type == 'regression'\n",
        "        score = sklearn.metrics.mean_squared_error(target, prediction) ** 0.5 * y_std\n",
        "    return score\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device=device)\n",
        "\n",
        "progress = zero.ProgressTracker(patience=100)\n",
        "\n",
        "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Obp6_vqvufM",
        "outputId": "f6142c8f-e31c-42a5-95ae-37ad0ba8b50a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score before training: 0.5094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2000\n",
        "report_frequency = len(X['train']) // batch_size // 5\n",
        "loss_lst=[]\n",
        "trainacc=[]\n",
        "testacc=[]\n",
        "valacc=[]\n",
        "len_trainloader=len(train_loader)\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    avg_loss=0\n",
        "    for iteration, batch_idx in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        x_batch = X['train'][batch_idx]\n",
        "        y_batch = y['train'][batch_idx]\n",
        "        loss = loss_fn(apply_model(x_batch).squeeze(1), y_batch.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "    avg_loss=avg_loss / len_trainloader\n",
        "    loss_lst.append(loss.item())\n",
        "    train_score = evaluate('train')\n",
        "    trainacc.append(train_score)\n",
        "    test_score = evaluate('test')\n",
        "    testacc.append(test_score)\n",
        "    print(f'Epoch {epoch:03d} | loss: {avg_loss:.4f} | train: {train_score:.4f} | Test score: {test_score:.4f}')\n",
        "    # progress.update((-1 if task_type == 'regression' else 1) * val_score)\n",
        "    # if progress.success:\n",
        "    #     print(' <<< BEST VALIDATION EPOCH', end='')\n",
        "    # print()\n",
        "    # if progress.fail:\n",
        "    #     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5RTQTH_vzJ2",
        "outputId": "933a42de-5bd2-4599-b284-b7164d248bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss: 0.6935 | train: 0.5457 | Test score: 0.5640\n",
            "Epoch 002 | loss: 0.6878 | train: 0.5499 | Test score: 0.5705\n",
            "Epoch 003 | loss: 0.6866 | train: 0.5531 | Test score: 0.5723\n",
            "Epoch 004 | loss: 0.6837 | train: 0.5548 | Test score: 0.5680\n",
            "Epoch 005 | loss: 0.6843 | train: 0.5564 | Test score: 0.5748\n",
            "Epoch 006 | loss: 0.6832 | train: 0.5557 | Test score: 0.5742\n",
            "Epoch 007 | loss: 0.6831 | train: 0.5557 | Test score: 0.5732\n",
            "Epoch 008 | loss: 0.6824 | train: 0.5543 | Test score: 0.5739\n",
            "Epoch 009 | loss: 0.6819 | train: 0.5559 | Test score: 0.5785\n",
            "Epoch 010 | loss: 0.6810 | train: 0.5576 | Test score: 0.5831\n",
            "Epoch 011 | loss: 0.6811 | train: 0.5564 | Test score: 0.5865\n",
            "Epoch 012 | loss: 0.6809 | train: 0.5569 | Test score: 0.5794\n",
            "Epoch 013 | loss: 0.6801 | train: 0.5564 | Test score: 0.5782\n",
            "Epoch 014 | loss: 0.6801 | train: 0.5575 | Test score: 0.5828\n",
            "Epoch 015 | loss: 0.6800 | train: 0.5575 | Test score: 0.5800\n",
            "Epoch 016 | loss: 0.6793 | train: 0.5578 | Test score: 0.5868\n",
            "Epoch 017 | loss: 0.6789 | train: 0.5577 | Test score: 0.5877\n",
            "Epoch 018 | loss: 0.6786 | train: 0.5595 | Test score: 0.5874\n",
            "Epoch 019 | loss: 0.6785 | train: 0.5578 | Test score: 0.5850\n",
            "Epoch 020 | loss: 0.6782 | train: 0.5585 | Test score: 0.5809\n",
            "Epoch 021 | loss: 0.6779 | train: 0.5584 | Test score: 0.5816\n",
            "Epoch 022 | loss: 0.6778 | train: 0.5580 | Test score: 0.5837\n",
            "Epoch 023 | loss: 0.6775 | train: 0.5598 | Test score: 0.5843\n",
            "Epoch 024 | loss: 0.6764 | train: 0.5600 | Test score: 0.5843\n",
            "Epoch 025 | loss: 0.6755 | train: 0.5610 | Test score: 0.5840\n",
            "Epoch 026 | loss: 0.6758 | train: 0.5602 | Test score: 0.5846\n",
            "Epoch 027 | loss: 0.6765 | train: 0.5613 | Test score: 0.5834\n",
            "Epoch 028 | loss: 0.6764 | train: 0.5614 | Test score: 0.5806\n",
            "Epoch 029 | loss: 0.6754 | train: 0.5620 | Test score: 0.5791\n",
            "Epoch 030 | loss: 0.6748 | train: 0.5608 | Test score: 0.5776\n",
            "Epoch 031 | loss: 0.6740 | train: 0.5619 | Test score: 0.5745\n",
            "Epoch 032 | loss: 0.6744 | train: 0.5624 | Test score: 0.5720\n",
            "Epoch 033 | loss: 0.6741 | train: 0.5636 | Test score: 0.5689\n",
            "Epoch 034 | loss: 0.6743 | train: 0.5632 | Test score: 0.5714\n",
            "Epoch 035 | loss: 0.6740 | train: 0.5650 | Test score: 0.5677\n",
            "Epoch 036 | loss: 0.6727 | train: 0.5653 | Test score: 0.5689\n",
            "Epoch 037 | loss: 0.6723 | train: 0.5650 | Test score: 0.5674\n",
            "Epoch 038 | loss: 0.6725 | train: 0.5657 | Test score: 0.5668\n",
            "Epoch 039 | loss: 0.6715 | train: 0.5665 | Test score: 0.5646\n",
            "Epoch 040 | loss: 0.6714 | train: 0.5659 | Test score: 0.5692\n",
            "Epoch 041 | loss: 0.6710 | train: 0.5661 | Test score: 0.5631\n",
            "Epoch 042 | loss: 0.6713 | train: 0.5683 | Test score: 0.5637\n",
            "Epoch 043 | loss: 0.6704 | train: 0.5709 | Test score: 0.5637\n",
            "Epoch 044 | loss: 0.6693 | train: 0.5693 | Test score: 0.5609\n",
            "Epoch 045 | loss: 0.6693 | train: 0.5696 | Test score: 0.5612\n",
            "Epoch 046 | loss: 0.6698 | train: 0.5707 | Test score: 0.5621\n",
            "Epoch 047 | loss: 0.6690 | train: 0.5716 | Test score: 0.5631\n",
            "Epoch 048 | loss: 0.6677 | train: 0.5715 | Test score: 0.5618\n",
            "Epoch 049 | loss: 0.6685 | train: 0.5740 | Test score: 0.5652\n",
            "Epoch 050 | loss: 0.6672 | train: 0.5729 | Test score: 0.5658\n",
            "Epoch 051 | loss: 0.6670 | train: 0.5723 | Test score: 0.5634\n",
            "Epoch 052 | loss: 0.6668 | train: 0.5740 | Test score: 0.5621\n",
            "Epoch 053 | loss: 0.6660 | train: 0.5749 | Test score: 0.5658\n",
            "Epoch 054 | loss: 0.6662 | train: 0.5723 | Test score: 0.5600\n",
            "Epoch 055 | loss: 0.6656 | train: 0.5729 | Test score: 0.5624\n",
            "Epoch 056 | loss: 0.6645 | train: 0.5728 | Test score: 0.5609\n",
            "Epoch 057 | loss: 0.6652 | train: 0.5728 | Test score: 0.5609\n",
            "Epoch 058 | loss: 0.6651 | train: 0.5748 | Test score: 0.5615\n",
            "Epoch 059 | loss: 0.6636 | train: 0.5766 | Test score: 0.5609\n",
            "Epoch 060 | loss: 0.6640 | train: 0.5759 | Test score: 0.5597\n",
            "Epoch 061 | loss: 0.6633 | train: 0.5758 | Test score: 0.5631\n",
            "Epoch 062 | loss: 0.6619 | train: 0.5766 | Test score: 0.5655\n",
            "Epoch 063 | loss: 0.6624 | train: 0.5793 | Test score: 0.5757\n",
            "Epoch 064 | loss: 0.6615 | train: 0.5793 | Test score: 0.5612\n",
            "Epoch 065 | loss: 0.6619 | train: 0.5782 | Test score: 0.5668\n",
            "Epoch 066 | loss: 0.6610 | train: 0.5793 | Test score: 0.5692\n",
            "Epoch 067 | loss: 0.6615 | train: 0.5792 | Test score: 0.5711\n",
            "Epoch 068 | loss: 0.6598 | train: 0.5803 | Test score: 0.5748\n",
            "Epoch 069 | loss: 0.6610 | train: 0.5807 | Test score: 0.5757\n",
            "Epoch 070 | loss: 0.6598 | train: 0.5816 | Test score: 0.5581\n",
            "Epoch 071 | loss: 0.6595 | train: 0.5803 | Test score: 0.5628\n",
            "Epoch 072 | loss: 0.6582 | train: 0.5805 | Test score: 0.5720\n",
            "Epoch 073 | loss: 0.6592 | train: 0.5805 | Test score: 0.5748\n",
            "Epoch 074 | loss: 0.6581 | train: 0.5819 | Test score: 0.5735\n",
            "Epoch 075 | loss: 0.6567 | train: 0.5812 | Test score: 0.5754\n",
            "Epoch 076 | loss: 0.6564 | train: 0.5803 | Test score: 0.5720\n",
            "Epoch 077 | loss: 0.6567 | train: 0.5828 | Test score: 0.5745\n",
            "Epoch 078 | loss: 0.6552 | train: 0.5837 | Test score: 0.5714\n",
            "Epoch 079 | loss: 0.6563 | train: 0.5859 | Test score: 0.5686\n",
            "Epoch 080 | loss: 0.6557 | train: 0.5848 | Test score: 0.5686\n",
            "Epoch 081 | loss: 0.6542 | train: 0.5840 | Test score: 0.5563\n",
            "Epoch 082 | loss: 0.6550 | train: 0.5843 | Test score: 0.5615\n",
            "Epoch 083 | loss: 0.6547 | train: 0.5874 | Test score: 0.5655\n",
            "Epoch 084 | loss: 0.6537 | train: 0.5863 | Test score: 0.5609\n",
            "Epoch 085 | loss: 0.6535 | train: 0.5850 | Test score: 0.5628\n",
            "Epoch 086 | loss: 0.6535 | train: 0.5863 | Test score: 0.5581\n",
            "Epoch 087 | loss: 0.6525 | train: 0.5852 | Test score: 0.5628\n",
            "Epoch 088 | loss: 0.6517 | train: 0.5838 | Test score: 0.5624\n",
            "Epoch 089 | loss: 0.6528 | train: 0.5849 | Test score: 0.5572\n",
            "Epoch 090 | loss: 0.6505 | train: 0.5869 | Test score: 0.5621\n",
            "Epoch 091 | loss: 0.6514 | train: 0.5851 | Test score: 0.5618\n",
            "Epoch 092 | loss: 0.6518 | train: 0.5847 | Test score: 0.5535\n",
            "Epoch 093 | loss: 0.6505 | train: 0.5873 | Test score: 0.5569\n",
            "Epoch 094 | loss: 0.6488 | train: 0.5870 | Test score: 0.5557\n",
            "Epoch 095 | loss: 0.6497 | train: 0.5844 | Test score: 0.5544\n",
            "Epoch 096 | loss: 0.6481 | train: 0.5872 | Test score: 0.5538\n",
            "Epoch 097 | loss: 0.6498 | train: 0.5834 | Test score: 0.5553\n",
            "Epoch 098 | loss: 0.6490 | train: 0.5851 | Test score: 0.5569\n",
            "Epoch 099 | loss: 0.6486 | train: 0.5857 | Test score: 0.5547\n",
            "Epoch 100 | loss: 0.6483 | train: 0.5884 | Test score: 0.5483\n",
            "Epoch 101 | loss: 0.6486 | train: 0.5869 | Test score: 0.5489\n",
            "Epoch 102 | loss: 0.6481 | train: 0.5885 | Test score: 0.5501\n",
            "Epoch 103 | loss: 0.6474 | train: 0.5884 | Test score: 0.5563\n",
            "Epoch 104 | loss: 0.6461 | train: 0.5876 | Test score: 0.5442\n",
            "Epoch 105 | loss: 0.6450 | train: 0.5874 | Test score: 0.5560\n",
            "Epoch 106 | loss: 0.6449 | train: 0.5894 | Test score: 0.5455\n",
            "Epoch 107 | loss: 0.6444 | train: 0.5885 | Test score: 0.5495\n",
            "Epoch 108 | loss: 0.6441 | train: 0.5885 | Test score: 0.5504\n",
            "Epoch 109 | loss: 0.6443 | train: 0.5886 | Test score: 0.5587\n",
            "Epoch 110 | loss: 0.6442 | train: 0.5891 | Test score: 0.5492\n",
            "Epoch 111 | loss: 0.6437 | train: 0.5865 | Test score: 0.5594\n",
            "Epoch 112 | loss: 0.6438 | train: 0.5870 | Test score: 0.5550\n",
            "Epoch 113 | loss: 0.6412 | train: 0.5862 | Test score: 0.5538\n",
            "Epoch 114 | loss: 0.6419 | train: 0.5889 | Test score: 0.5492\n",
            "Epoch 115 | loss: 0.6426 | train: 0.5890 | Test score: 0.5483\n",
            "Epoch 116 | loss: 0.6415 | train: 0.5889 | Test score: 0.5489\n",
            "Epoch 117 | loss: 0.6412 | train: 0.5863 | Test score: 0.5486\n",
            "Epoch 118 | loss: 0.6392 | train: 0.5889 | Test score: 0.5495\n",
            "Epoch 119 | loss: 0.6414 | train: 0.5892 | Test score: 0.5473\n",
            "Epoch 120 | loss: 0.6392 | train: 0.5896 | Test score: 0.5455\n",
            "Epoch 121 | loss: 0.6393 | train: 0.5902 | Test score: 0.5541\n",
            "Epoch 122 | loss: 0.6386 | train: 0.5900 | Test score: 0.5449\n",
            "Epoch 123 | loss: 0.6382 | train: 0.5847 | Test score: 0.5384\n",
            "Epoch 124 | loss: 0.6382 | train: 0.5872 | Test score: 0.5402\n",
            "Epoch 125 | loss: 0.6381 | train: 0.5891 | Test score: 0.5449\n",
            "Epoch 126 | loss: 0.6378 | train: 0.5868 | Test score: 0.5504\n",
            "Epoch 127 | loss: 0.6376 | train: 0.5867 | Test score: 0.5424\n",
            "Epoch 128 | loss: 0.6362 | train: 0.5861 | Test score: 0.5430\n",
            "Epoch 129 | loss: 0.6365 | train: 0.5856 | Test score: 0.5458\n",
            "Epoch 130 | loss: 0.6372 | train: 0.5866 | Test score: 0.5439\n",
            "Epoch 131 | loss: 0.6348 | train: 0.5850 | Test score: 0.5464\n",
            "Epoch 132 | loss: 0.6363 | train: 0.5868 | Test score: 0.5532\n",
            "Epoch 133 | loss: 0.6344 | train: 0.5870 | Test score: 0.5455\n",
            "Epoch 134 | loss: 0.6356 | train: 0.5857 | Test score: 0.5433\n",
            "Epoch 135 | loss: 0.6354 | train: 0.5837 | Test score: 0.5384\n",
            "Epoch 136 | loss: 0.6353 | train: 0.5868 | Test score: 0.5433\n",
            "Epoch 137 | loss: 0.6345 | train: 0.5837 | Test score: 0.5409\n",
            "Epoch 138 | loss: 0.6331 | train: 0.5844 | Test score: 0.5415\n",
            "Epoch 139 | loss: 0.6329 | train: 0.5831 | Test score: 0.5375\n",
            "Epoch 140 | loss: 0.6335 | train: 0.5847 | Test score: 0.5433\n",
            "Epoch 141 | loss: 0.6320 | train: 0.5842 | Test score: 0.5390\n",
            "Epoch 142 | loss: 0.6319 | train: 0.5837 | Test score: 0.5294\n",
            "Epoch 143 | loss: 0.6320 | train: 0.5829 | Test score: 0.5341\n",
            "Epoch 144 | loss: 0.6315 | train: 0.5826 | Test score: 0.5405\n",
            "Epoch 145 | loss: 0.6305 | train: 0.5836 | Test score: 0.5378\n",
            "Epoch 146 | loss: 0.6305 | train: 0.5882 | Test score: 0.5409\n",
            "Epoch 147 | loss: 0.6295 | train: 0.5856 | Test score: 0.5436\n",
            "Epoch 148 | loss: 0.6295 | train: 0.5852 | Test score: 0.5442\n",
            "Epoch 149 | loss: 0.6289 | train: 0.5812 | Test score: 0.5532\n",
            "Epoch 150 | loss: 0.6298 | train: 0.5833 | Test score: 0.5464\n",
            "Epoch 151 | loss: 0.6287 | train: 0.5838 | Test score: 0.5430\n",
            "Epoch 152 | loss: 0.6274 | train: 0.5848 | Test score: 0.5461\n",
            "Epoch 153 | loss: 0.6261 | train: 0.5866 | Test score: 0.5387\n",
            "Epoch 154 | loss: 0.6274 | train: 0.5827 | Test score: 0.5402\n",
            "Epoch 155 | loss: 0.6262 | train: 0.5836 | Test score: 0.5350\n",
            "Epoch 156 | loss: 0.6251 | train: 0.5831 | Test score: 0.5409\n",
            "Epoch 157 | loss: 0.6251 | train: 0.5839 | Test score: 0.5470\n",
            "Epoch 158 | loss: 0.6260 | train: 0.5843 | Test score: 0.5455\n",
            "Epoch 159 | loss: 0.6249 | train: 0.5849 | Test score: 0.5439\n",
            "Epoch 160 | loss: 0.6253 | train: 0.5838 | Test score: 0.5359\n",
            "Epoch 161 | loss: 0.6255 | train: 0.5849 | Test score: 0.5390\n",
            "Epoch 162 | loss: 0.6256 | train: 0.5842 | Test score: 0.5421\n",
            "Epoch 163 | loss: 0.6245 | train: 0.5846 | Test score: 0.5390\n",
            "Epoch 164 | loss: 0.6245 | train: 0.5822 | Test score: 0.5455\n",
            "Epoch 165 | loss: 0.6205 | train: 0.5852 | Test score: 0.5461\n",
            "Epoch 166 | loss: 0.6208 | train: 0.5854 | Test score: 0.5467\n",
            "Epoch 167 | loss: 0.6212 | train: 0.5842 | Test score: 0.5393\n",
            "Epoch 168 | loss: 0.6211 | train: 0.5846 | Test score: 0.5424\n",
            "Epoch 169 | loss: 0.6208 | train: 0.5848 | Test score: 0.5325\n",
            "Epoch 170 | loss: 0.6208 | train: 0.5856 | Test score: 0.5356\n",
            "Epoch 171 | loss: 0.6179 | train: 0.5855 | Test score: 0.5489\n",
            "Epoch 172 | loss: 0.6203 | train: 0.5844 | Test score: 0.5483\n",
            "Epoch 173 | loss: 0.6207 | train: 0.5835 | Test score: 0.5409\n",
            "Epoch 174 | loss: 0.6172 | train: 0.5848 | Test score: 0.5331\n",
            "Epoch 175 | loss: 0.6191 | train: 0.5865 | Test score: 0.5381\n",
            "Epoch 176 | loss: 0.6170 | train: 0.5838 | Test score: 0.5381\n",
            "Epoch 177 | loss: 0.6178 | train: 0.5857 | Test score: 0.5467\n",
            "Epoch 178 | loss: 0.6167 | train: 0.5861 | Test score: 0.5372\n",
            "Epoch 179 | loss: 0.6181 | train: 0.5852 | Test score: 0.5368\n",
            "Epoch 180 | loss: 0.6178 | train: 0.5862 | Test score: 0.5402\n",
            "Epoch 181 | loss: 0.6161 | train: 0.5850 | Test score: 0.5372\n",
            "Epoch 182 | loss: 0.6169 | train: 0.5859 | Test score: 0.5458\n",
            "Epoch 183 | loss: 0.6153 | train: 0.5841 | Test score: 0.5507\n",
            "Epoch 184 | loss: 0.6152 | train: 0.5844 | Test score: 0.5495\n",
            "Epoch 185 | loss: 0.6161 | train: 0.5867 | Test score: 0.5368\n",
            "Epoch 186 | loss: 0.6145 | train: 0.5872 | Test score: 0.5427\n",
            "Epoch 187 | loss: 0.6138 | train: 0.5891 | Test score: 0.5455\n",
            "Epoch 188 | loss: 0.6141 | train: 0.5842 | Test score: 0.5476\n",
            "Epoch 189 | loss: 0.6134 | train: 0.5860 | Test score: 0.5507\n",
            "Epoch 190 | loss: 0.6122 | train: 0.5865 | Test score: 0.5489\n",
            "Epoch 191 | loss: 0.6144 | train: 0.5843 | Test score: 0.5458\n",
            "Epoch 192 | loss: 0.6136 | train: 0.5859 | Test score: 0.5501\n",
            "Epoch 193 | loss: 0.6132 | train: 0.5838 | Test score: 0.5449\n",
            "Epoch 194 | loss: 0.6136 | train: 0.5854 | Test score: 0.5424\n",
            "Epoch 195 | loss: 0.6117 | train: 0.5859 | Test score: 0.5470\n",
            "Epoch 196 | loss: 0.6107 | train: 0.5848 | Test score: 0.5566\n",
            "Epoch 197 | loss: 0.6105 | train: 0.5858 | Test score: 0.5319\n",
            "Epoch 198 | loss: 0.6099 | train: 0.5844 | Test score: 0.5430\n",
            "Epoch 199 | loss: 0.6141 | train: 0.5846 | Test score: 0.5430\n",
            "Epoch 200 | loss: 0.6100 | train: 0.5848 | Test score: 0.5483\n",
            "Epoch 201 | loss: 0.6098 | train: 0.5854 | Test score: 0.5483\n",
            "Epoch 202 | loss: 0.6105 | train: 0.5847 | Test score: 0.5405\n",
            "Epoch 203 | loss: 0.6109 | train: 0.5831 | Test score: 0.5439\n",
            "Epoch 204 | loss: 0.6089 | train: 0.5858 | Test score: 0.5381\n",
            "Epoch 205 | loss: 0.6092 | train: 0.5839 | Test score: 0.5467\n",
            "Epoch 206 | loss: 0.6060 | train: 0.5831 | Test score: 0.5436\n",
            "Epoch 207 | loss: 0.6093 | train: 0.5834 | Test score: 0.5458\n",
            "Epoch 208 | loss: 0.6076 | train: 0.5852 | Test score: 0.5476\n",
            "Epoch 209 | loss: 0.6066 | train: 0.5850 | Test score: 0.5439\n",
            "Epoch 210 | loss: 0.6068 | train: 0.5849 | Test score: 0.5433\n",
            "Epoch 211 | loss: 0.6039 | train: 0.5847 | Test score: 0.5433\n",
            "Epoch 212 | loss: 0.6058 | train: 0.5871 | Test score: 0.5560\n",
            "Epoch 213 | loss: 0.6052 | train: 0.5844 | Test score: 0.5378\n",
            "Epoch 214 | loss: 0.6075 | train: 0.5860 | Test score: 0.5507\n",
            "Epoch 215 | loss: 0.6066 | train: 0.5847 | Test score: 0.5418\n",
            "Epoch 216 | loss: 0.6047 | train: 0.5868 | Test score: 0.5375\n",
            "Epoch 217 | loss: 0.6039 | train: 0.5861 | Test score: 0.5384\n",
            "Epoch 218 | loss: 0.6045 | train: 0.5828 | Test score: 0.5402\n",
            "Epoch 219 | loss: 0.6031 | train: 0.5840 | Test score: 0.5455\n",
            "Epoch 220 | loss: 0.6050 | train: 0.5820 | Test score: 0.5412\n",
            "Epoch 221 | loss: 0.6008 | train: 0.5853 | Test score: 0.5390\n",
            "Epoch 222 | loss: 0.6023 | train: 0.5849 | Test score: 0.5421\n",
            "Epoch 223 | loss: 0.6033 | train: 0.5848 | Test score: 0.5402\n",
            "Epoch 224 | loss: 0.6010 | train: 0.5862 | Test score: 0.5384\n",
            "Epoch 225 | loss: 0.5996 | train: 0.5843 | Test score: 0.5483\n",
            "Epoch 226 | loss: 0.6021 | train: 0.5840 | Test score: 0.5461\n",
            "Epoch 227 | loss: 0.6014 | train: 0.5848 | Test score: 0.5483\n",
            "Epoch 228 | loss: 0.6013 | train: 0.5834 | Test score: 0.5504\n",
            "Epoch 229 | loss: 0.5989 | train: 0.5810 | Test score: 0.5396\n",
            "Epoch 230 | loss: 0.5994 | train: 0.5838 | Test score: 0.5368\n",
            "Epoch 231 | loss: 0.6009 | train: 0.5856 | Test score: 0.5449\n",
            "Epoch 232 | loss: 0.5984 | train: 0.5834 | Test score: 0.5455\n",
            "Epoch 233 | loss: 0.5999 | train: 0.5832 | Test score: 0.5427\n",
            "Epoch 234 | loss: 0.6005 | train: 0.5831 | Test score: 0.5467\n",
            "Epoch 235 | loss: 0.6006 | train: 0.5839 | Test score: 0.5356\n",
            "Epoch 236 | loss: 0.5987 | train: 0.5863 | Test score: 0.5430\n",
            "Epoch 237 | loss: 0.5962 | train: 0.5855 | Test score: 0.5504\n",
            "Epoch 238 | loss: 0.5984 | train: 0.5821 | Test score: 0.5436\n",
            "Epoch 239 | loss: 0.5994 | train: 0.5823 | Test score: 0.5455\n",
            "Epoch 240 | loss: 0.5958 | train: 0.5865 | Test score: 0.5523\n",
            "Epoch 241 | loss: 0.5943 | train: 0.5852 | Test score: 0.5409\n",
            "Epoch 242 | loss: 0.5965 | train: 0.5847 | Test score: 0.5442\n",
            "Epoch 243 | loss: 0.5958 | train: 0.5820 | Test score: 0.5446\n",
            "Epoch 244 | loss: 0.5946 | train: 0.5833 | Test score: 0.5387\n",
            "Epoch 245 | loss: 0.5951 | train: 0.5862 | Test score: 0.5381\n",
            "Epoch 246 | loss: 0.5933 | train: 0.5837 | Test score: 0.5424\n",
            "Epoch 247 | loss: 0.5945 | train: 0.5860 | Test score: 0.5412\n",
            "Epoch 248 | loss: 0.5948 | train: 0.5887 | Test score: 0.5405\n",
            "Epoch 249 | loss: 0.5947 | train: 0.5871 | Test score: 0.5483\n",
            "Epoch 250 | loss: 0.5914 | train: 0.5849 | Test score: 0.5372\n",
            "Epoch 251 | loss: 0.5945 | train: 0.5838 | Test score: 0.5461\n",
            "Epoch 252 | loss: 0.5918 | train: 0.5854 | Test score: 0.5421\n",
            "Epoch 253 | loss: 0.5939 | train: 0.5857 | Test score: 0.5464\n",
            "Epoch 254 | loss: 0.5917 | train: 0.5864 | Test score: 0.5507\n",
            "Epoch 255 | loss: 0.5910 | train: 0.5850 | Test score: 0.5415\n",
            "Epoch 256 | loss: 0.5904 | train: 0.5849 | Test score: 0.5473\n",
            "Epoch 257 | loss: 0.5931 | train: 0.5833 | Test score: 0.5516\n",
            "Epoch 258 | loss: 0.5905 | train: 0.5849 | Test score: 0.5446\n",
            "Epoch 259 | loss: 0.5926 | train: 0.5858 | Test score: 0.5529\n",
            "Epoch 260 | loss: 0.5869 | train: 0.5868 | Test score: 0.5507\n",
            "Epoch 261 | loss: 0.5892 | train: 0.5839 | Test score: 0.5486\n",
            "Epoch 262 | loss: 0.5898 | train: 0.5842 | Test score: 0.5449\n",
            "Epoch 263 | loss: 0.5895 | train: 0.5841 | Test score: 0.5486\n",
            "Epoch 264 | loss: 0.5896 | train: 0.5859 | Test score: 0.5458\n",
            "Epoch 265 | loss: 0.5866 | train: 0.5832 | Test score: 0.5461\n",
            "Epoch 266 | loss: 0.5865 | train: 0.5832 | Test score: 0.5495\n",
            "Epoch 267 | loss: 0.5887 | train: 0.5852 | Test score: 0.5310\n",
            "Epoch 268 | loss: 0.5884 | train: 0.5844 | Test score: 0.5396\n",
            "Epoch 269 | loss: 0.5883 | train: 0.5844 | Test score: 0.5523\n",
            "Epoch 270 | loss: 0.5876 | train: 0.5856 | Test score: 0.5446\n",
            "Epoch 271 | loss: 0.5874 | train: 0.5846 | Test score: 0.5381\n",
            "Epoch 272 | loss: 0.5852 | train: 0.5843 | Test score: 0.5464\n",
            "Epoch 273 | loss: 0.5879 | train: 0.5856 | Test score: 0.5541\n",
            "Epoch 274 | loss: 0.5828 | train: 0.5829 | Test score: 0.5430\n",
            "Epoch 275 | loss: 0.5844 | train: 0.5831 | Test score: 0.5442\n",
            "Epoch 276 | loss: 0.5828 | train: 0.5851 | Test score: 0.5501\n",
            "Epoch 277 | loss: 0.5864 | train: 0.5835 | Test score: 0.5526\n",
            "Epoch 278 | loss: 0.5840 | train: 0.5851 | Test score: 0.5476\n",
            "Epoch 279 | loss: 0.5857 | train: 0.5837 | Test score: 0.5507\n",
            "Epoch 280 | loss: 0.5861 | train: 0.5866 | Test score: 0.5427\n",
            "Epoch 281 | loss: 0.5839 | train: 0.5855 | Test score: 0.5436\n",
            "Epoch 282 | loss: 0.5827 | train: 0.5872 | Test score: 0.5418\n",
            "Epoch 283 | loss: 0.5827 | train: 0.5847 | Test score: 0.5446\n",
            "Epoch 284 | loss: 0.5824 | train: 0.5867 | Test score: 0.5458\n",
            "Epoch 285 | loss: 0.5821 | train: 0.5845 | Test score: 0.5458\n",
            "Epoch 286 | loss: 0.5814 | train: 0.5889 | Test score: 0.5384\n",
            "Epoch 287 | loss: 0.5819 | train: 0.5840 | Test score: 0.5489\n",
            "Epoch 288 | loss: 0.5845 | train: 0.5871 | Test score: 0.5378\n",
            "Epoch 289 | loss: 0.5810 | train: 0.5873 | Test score: 0.5449\n",
            "Epoch 290 | loss: 0.5795 | train: 0.5889 | Test score: 0.5319\n",
            "Epoch 291 | loss: 0.5809 | train: 0.5861 | Test score: 0.5489\n",
            "Epoch 292 | loss: 0.5778 | train: 0.5862 | Test score: 0.5409\n",
            "Epoch 293 | loss: 0.5819 | train: 0.5871 | Test score: 0.5430\n",
            "Epoch 294 | loss: 0.5795 | train: 0.5880 | Test score: 0.5507\n",
            "Epoch 295 | loss: 0.5778 | train: 0.5865 | Test score: 0.5399\n",
            "Epoch 296 | loss: 0.5770 | train: 0.5850 | Test score: 0.5347\n",
            "Epoch 297 | loss: 0.5768 | train: 0.5851 | Test score: 0.5427\n",
            "Epoch 298 | loss: 0.5761 | train: 0.5870 | Test score: 0.5362\n",
            "Epoch 299 | loss: 0.5786 | train: 0.5887 | Test score: 0.5224\n",
            "Epoch 300 | loss: 0.5774 | train: 0.5852 | Test score: 0.5433\n",
            "Epoch 301 | loss: 0.5762 | train: 0.5866 | Test score: 0.5353\n",
            "Epoch 302 | loss: 0.5761 | train: 0.5822 | Test score: 0.5353\n",
            "Epoch 303 | loss: 0.5759 | train: 0.5851 | Test score: 0.5304\n",
            "Epoch 304 | loss: 0.5771 | train: 0.5883 | Test score: 0.5390\n",
            "Epoch 305 | loss: 0.5762 | train: 0.5805 | Test score: 0.5492\n",
            "Epoch 306 | loss: 0.5764 | train: 0.5826 | Test score: 0.5476\n",
            "Epoch 307 | loss: 0.5770 | train: 0.5862 | Test score: 0.5378\n",
            "Epoch 308 | loss: 0.5758 | train: 0.5875 | Test score: 0.5313\n",
            "Epoch 309 | loss: 0.5734 | train: 0.5892 | Test score: 0.5325\n",
            "Epoch 310 | loss: 0.5741 | train: 0.5874 | Test score: 0.5338\n",
            "Epoch 311 | loss: 0.5736 | train: 0.5855 | Test score: 0.5325\n",
            "Epoch 312 | loss: 0.5764 | train: 0.5864 | Test score: 0.5270\n",
            "Epoch 313 | loss: 0.5736 | train: 0.5872 | Test score: 0.5310\n",
            "Epoch 314 | loss: 0.5747 | train: 0.5850 | Test score: 0.5375\n",
            "Epoch 315 | loss: 0.5722 | train: 0.5884 | Test score: 0.5368\n",
            "Epoch 316 | loss: 0.5711 | train: 0.5883 | Test score: 0.5412\n",
            "Epoch 317 | loss: 0.5733 | train: 0.5885 | Test score: 0.5424\n",
            "Epoch 318 | loss: 0.5735 | train: 0.5891 | Test score: 0.5359\n",
            "Epoch 319 | loss: 0.5712 | train: 0.5844 | Test score: 0.5436\n",
            "Epoch 320 | loss: 0.5697 | train: 0.5886 | Test score: 0.5479\n",
            "Epoch 321 | loss: 0.5712 | train: 0.5890 | Test score: 0.5405\n",
            "Epoch 322 | loss: 0.5694 | train: 0.5842 | Test score: 0.5378\n",
            "Epoch 323 | loss: 0.5713 | train: 0.5887 | Test score: 0.5313\n",
            "Epoch 324 | loss: 0.5688 | train: 0.5861 | Test score: 0.5298\n",
            "Epoch 325 | loss: 0.5696 | train: 0.5878 | Test score: 0.5350\n",
            "Epoch 326 | loss: 0.5699 | train: 0.5889 | Test score: 0.5455\n",
            "Epoch 327 | loss: 0.5692 | train: 0.5891 | Test score: 0.5294\n",
            "Epoch 328 | loss: 0.5706 | train: 0.5911 | Test score: 0.5353\n",
            "Epoch 329 | loss: 0.5696 | train: 0.5879 | Test score: 0.5214\n",
            "Epoch 330 | loss: 0.5724 | train: 0.5881 | Test score: 0.5282\n",
            "Epoch 331 | loss: 0.5689 | train: 0.5854 | Test score: 0.5261\n",
            "Epoch 332 | loss: 0.5679 | train: 0.5884 | Test score: 0.5335\n",
            "Epoch 333 | loss: 0.5672 | train: 0.5888 | Test score: 0.5279\n",
            "Epoch 334 | loss: 0.5709 | train: 0.5873 | Test score: 0.5353\n",
            "Epoch 335 | loss: 0.5663 | train: 0.5876 | Test score: 0.5362\n",
            "Epoch 336 | loss: 0.5671 | train: 0.5863 | Test score: 0.5245\n",
            "Epoch 337 | loss: 0.5653 | train: 0.5881 | Test score: 0.5322\n",
            "Epoch 338 | loss: 0.5647 | train: 0.5859 | Test score: 0.5267\n",
            "Epoch 339 | loss: 0.5632 | train: 0.5886 | Test score: 0.5180\n",
            "Epoch 340 | loss: 0.5648 | train: 0.5865 | Test score: 0.5344\n",
            "Epoch 341 | loss: 0.5656 | train: 0.5855 | Test score: 0.5236\n",
            "Epoch 342 | loss: 0.5665 | train: 0.5896 | Test score: 0.5341\n",
            "Epoch 343 | loss: 0.5633 | train: 0.5905 | Test score: 0.5298\n",
            "Epoch 344 | loss: 0.5639 | train: 0.5835 | Test score: 0.5319\n",
            "Epoch 345 | loss: 0.5627 | train: 0.5884 | Test score: 0.5236\n",
            "Epoch 346 | loss: 0.5666 | train: 0.5901 | Test score: 0.5245\n",
            "Epoch 347 | loss: 0.5627 | train: 0.5857 | Test score: 0.5202\n",
            "Epoch 348 | loss: 0.5616 | train: 0.5865 | Test score: 0.5220\n",
            "Epoch 349 | loss: 0.5607 | train: 0.5892 | Test score: 0.5193\n",
            "Epoch 350 | loss: 0.5651 | train: 0.5914 | Test score: 0.5270\n",
            "Epoch 351 | loss: 0.5617 | train: 0.5878 | Test score: 0.5150\n",
            "Epoch 352 | loss: 0.5598 | train: 0.5878 | Test score: 0.5190\n",
            "Epoch 353 | loss: 0.5593 | train: 0.5888 | Test score: 0.5248\n",
            "Epoch 354 | loss: 0.5610 | train: 0.5881 | Test score: 0.5273\n",
            "Epoch 355 | loss: 0.5592 | train: 0.5880 | Test score: 0.5328\n",
            "Epoch 356 | loss: 0.5587 | train: 0.5879 | Test score: 0.5375\n",
            "Epoch 357 | loss: 0.5621 | train: 0.5896 | Test score: 0.5150\n",
            "Epoch 358 | loss: 0.5642 | train: 0.5892 | Test score: 0.5211\n",
            "Epoch 359 | loss: 0.5593 | train: 0.5878 | Test score: 0.5313\n",
            "Epoch 360 | loss: 0.5607 | train: 0.5906 | Test score: 0.5239\n",
            "Epoch 361 | loss: 0.5580 | train: 0.5869 | Test score: 0.5294\n",
            "Epoch 362 | loss: 0.5621 | train: 0.5891 | Test score: 0.5214\n",
            "Epoch 363 | loss: 0.5579 | train: 0.5861 | Test score: 0.5199\n",
            "Epoch 364 | loss: 0.5590 | train: 0.5884 | Test score: 0.5282\n",
            "Epoch 365 | loss: 0.5598 | train: 0.5876 | Test score: 0.5347\n",
            "Epoch 366 | loss: 0.5564 | train: 0.5883 | Test score: 0.5208\n",
            "Epoch 367 | loss: 0.5598 | train: 0.5880 | Test score: 0.5427\n",
            "Epoch 368 | loss: 0.5559 | train: 0.5886 | Test score: 0.5254\n",
            "Epoch 369 | loss: 0.5578 | train: 0.5859 | Test score: 0.5202\n",
            "Epoch 370 | loss: 0.5559 | train: 0.5883 | Test score: 0.5264\n",
            "Epoch 371 | loss: 0.5566 | train: 0.5869 | Test score: 0.5331\n",
            "Epoch 372 | loss: 0.5546 | train: 0.5883 | Test score: 0.5273\n",
            "Epoch 373 | loss: 0.5545 | train: 0.5879 | Test score: 0.5424\n",
            "Epoch 374 | loss: 0.5570 | train: 0.5858 | Test score: 0.5279\n",
            "Epoch 375 | loss: 0.5535 | train: 0.5875 | Test score: 0.5344\n",
            "Epoch 376 | loss: 0.5536 | train: 0.5879 | Test score: 0.5316\n",
            "Epoch 377 | loss: 0.5543 | train: 0.5868 | Test score: 0.5270\n",
            "Epoch 378 | loss: 0.5546 | train: 0.5881 | Test score: 0.5356\n",
            "Epoch 379 | loss: 0.5516 | train: 0.5885 | Test score: 0.5242\n",
            "Epoch 380 | loss: 0.5527 | train: 0.5898 | Test score: 0.5313\n",
            "Epoch 381 | loss: 0.5526 | train: 0.5852 | Test score: 0.5325\n",
            "Epoch 382 | loss: 0.5544 | train: 0.5883 | Test score: 0.5242\n",
            "Epoch 383 | loss: 0.5557 | train: 0.5896 | Test score: 0.5276\n",
            "Epoch 384 | loss: 0.5548 | train: 0.5884 | Test score: 0.5254\n",
            "Epoch 385 | loss: 0.5507 | train: 0.5881 | Test score: 0.5230\n",
            "Epoch 386 | loss: 0.5534 | train: 0.5895 | Test score: 0.5372\n",
            "Epoch 387 | loss: 0.5497 | train: 0.5874 | Test score: 0.5254\n",
            "Epoch 388 | loss: 0.5503 | train: 0.5951 | Test score: 0.5248\n",
            "Epoch 389 | loss: 0.5473 | train: 0.5846 | Test score: 0.5356\n",
            "Epoch 390 | loss: 0.5526 | train: 0.5904 | Test score: 0.5304\n",
            "Epoch 391 | loss: 0.5529 | train: 0.5889 | Test score: 0.5301\n",
            "Epoch 392 | loss: 0.5508 | train: 0.5897 | Test score: 0.5273\n",
            "Epoch 393 | loss: 0.5530 | train: 0.5897 | Test score: 0.5335\n",
            "Epoch 394 | loss: 0.5510 | train: 0.5900 | Test score: 0.5270\n",
            "Epoch 395 | loss: 0.5476 | train: 0.5876 | Test score: 0.5254\n",
            "Epoch 396 | loss: 0.5499 | train: 0.5911 | Test score: 0.5285\n",
            "Epoch 397 | loss: 0.5473 | train: 0.5877 | Test score: 0.5313\n",
            "Epoch 398 | loss: 0.5489 | train: 0.5891 | Test score: 0.5316\n",
            "Epoch 399 | loss: 0.5489 | train: 0.5912 | Test score: 0.5211\n",
            "Epoch 400 | loss: 0.5489 | train: 0.5885 | Test score: 0.5270\n",
            "Epoch 401 | loss: 0.5460 | train: 0.5864 | Test score: 0.5270\n",
            "Epoch 402 | loss: 0.5469 | train: 0.5885 | Test score: 0.5350\n",
            "Epoch 403 | loss: 0.5491 | train: 0.5873 | Test score: 0.5220\n",
            "Epoch 404 | loss: 0.5448 | train: 0.5895 | Test score: 0.5245\n",
            "Epoch 405 | loss: 0.5498 | train: 0.5898 | Test score: 0.5341\n",
            "Epoch 406 | loss: 0.5447 | train: 0.5900 | Test score: 0.5288\n",
            "Epoch 407 | loss: 0.5481 | train: 0.5876 | Test score: 0.5291\n",
            "Epoch 408 | loss: 0.5445 | train: 0.5916 | Test score: 0.5261\n",
            "Epoch 409 | loss: 0.5485 | train: 0.5890 | Test score: 0.5335\n",
            "Epoch 410 | loss: 0.5449 | train: 0.5859 | Test score: 0.5415\n",
            "Epoch 411 | loss: 0.5452 | train: 0.5877 | Test score: 0.5183\n",
            "Epoch 412 | loss: 0.5437 | train: 0.5833 | Test score: 0.5196\n",
            "Epoch 413 | loss: 0.5433 | train: 0.5831 | Test score: 0.5307\n",
            "Epoch 414 | loss: 0.5442 | train: 0.5854 | Test score: 0.5279\n",
            "Epoch 415 | loss: 0.5430 | train: 0.5872 | Test score: 0.5270\n",
            "Epoch 416 | loss: 0.5432 | train: 0.5886 | Test score: 0.5319\n",
            "Epoch 417 | loss: 0.5442 | train: 0.5874 | Test score: 0.5310\n",
            "Epoch 418 | loss: 0.5426 | train: 0.5861 | Test score: 0.5307\n",
            "Epoch 419 | loss: 0.5444 | train: 0.5889 | Test score: 0.5384\n",
            "Epoch 420 | loss: 0.5437 | train: 0.5882 | Test score: 0.5372\n",
            "Epoch 421 | loss: 0.5435 | train: 0.5889 | Test score: 0.5242\n",
            "Epoch 422 | loss: 0.5415 | train: 0.5880 | Test score: 0.5365\n",
            "Epoch 423 | loss: 0.5428 | train: 0.5895 | Test score: 0.5264\n",
            "Epoch 424 | loss: 0.5442 | train: 0.5864 | Test score: 0.5344\n",
            "Epoch 425 | loss: 0.5425 | train: 0.5852 | Test score: 0.5313\n",
            "Epoch 426 | loss: 0.5419 | train: 0.5851 | Test score: 0.5239\n",
            "Epoch 427 | loss: 0.5432 | train: 0.5887 | Test score: 0.5270\n",
            "Epoch 428 | loss: 0.5433 | train: 0.5879 | Test score: 0.5344\n",
            "Epoch 429 | loss: 0.5407 | train: 0.5892 | Test score: 0.5270\n",
            "Epoch 430 | loss: 0.5388 | train: 0.5861 | Test score: 0.5338\n",
            "Epoch 431 | loss: 0.5405 | train: 0.5825 | Test score: 0.5310\n",
            "Epoch 432 | loss: 0.5380 | train: 0.5867 | Test score: 0.5322\n",
            "Epoch 433 | loss: 0.5391 | train: 0.5863 | Test score: 0.5298\n",
            "Epoch 434 | loss: 0.5418 | train: 0.5887 | Test score: 0.5245\n",
            "Epoch 435 | loss: 0.5411 | train: 0.5855 | Test score: 0.5294\n",
            "Epoch 436 | loss: 0.5386 | train: 0.5848 | Test score: 0.5319\n",
            "Epoch 437 | loss: 0.5372 | train: 0.5866 | Test score: 0.5368\n",
            "Epoch 438 | loss: 0.5377 | train: 0.5827 | Test score: 0.5526\n",
            "Epoch 439 | loss: 0.5378 | train: 0.5889 | Test score: 0.5381\n",
            "Epoch 440 | loss: 0.5350 | train: 0.5893 | Test score: 0.5304\n",
            "Epoch 441 | loss: 0.5378 | train: 0.5854 | Test score: 0.5282\n",
            "Epoch 442 | loss: 0.5396 | train: 0.5853 | Test score: 0.5338\n",
            "Epoch 443 | loss: 0.5362 | train: 0.5846 | Test score: 0.5217\n",
            "Epoch 444 | loss: 0.5374 | train: 0.5881 | Test score: 0.5495\n",
            "Epoch 445 | loss: 0.5364 | train: 0.5862 | Test score: 0.5257\n",
            "Epoch 446 | loss: 0.5355 | train: 0.5889 | Test score: 0.5298\n",
            "Epoch 447 | loss: 0.5350 | train: 0.5869 | Test score: 0.5322\n",
            "Epoch 448 | loss: 0.5340 | train: 0.5877 | Test score: 0.5288\n",
            "Epoch 449 | loss: 0.5328 | train: 0.5877 | Test score: 0.5264\n",
            "Epoch 450 | loss: 0.5374 | train: 0.5852 | Test score: 0.5412\n",
            "Epoch 451 | loss: 0.5354 | train: 0.5823 | Test score: 0.5427\n",
            "Epoch 452 | loss: 0.5347 | train: 0.5865 | Test score: 0.5362\n",
            "Epoch 453 | loss: 0.5352 | train: 0.5868 | Test score: 0.5359\n",
            "Epoch 454 | loss: 0.5328 | train: 0.5901 | Test score: 0.5322\n",
            "Epoch 455 | loss: 0.5350 | train: 0.5851 | Test score: 0.5245\n",
            "Epoch 456 | loss: 0.5304 | train: 0.5817 | Test score: 0.5362\n",
            "Epoch 457 | loss: 0.5342 | train: 0.5824 | Test score: 0.5390\n",
            "Epoch 458 | loss: 0.5325 | train: 0.5814 | Test score: 0.5412\n",
            "Epoch 459 | loss: 0.5310 | train: 0.5889 | Test score: 0.5365\n",
            "Epoch 460 | loss: 0.5342 | train: 0.5853 | Test score: 0.5405\n",
            "Epoch 461 | loss: 0.5329 | train: 0.5873 | Test score: 0.5387\n",
            "Epoch 462 | loss: 0.5323 | train: 0.5873 | Test score: 0.5365\n",
            "Epoch 463 | loss: 0.5324 | train: 0.5869 | Test score: 0.5338\n",
            "Epoch 464 | loss: 0.5331 | train: 0.5831 | Test score: 0.5254\n",
            "Epoch 465 | loss: 0.5328 | train: 0.5822 | Test score: 0.5405\n",
            "Epoch 466 | loss: 0.5298 | train: 0.5872 | Test score: 0.5304\n",
            "Epoch 467 | loss: 0.5299 | train: 0.5819 | Test score: 0.5495\n",
            "Epoch 468 | loss: 0.5291 | train: 0.5851 | Test score: 0.5298\n",
            "Epoch 469 | loss: 0.5297 | train: 0.5874 | Test score: 0.5205\n",
            "Epoch 470 | loss: 0.5296 | train: 0.5866 | Test score: 0.5220\n",
            "Epoch 471 | loss: 0.5308 | train: 0.5870 | Test score: 0.5313\n",
            "Epoch 472 | loss: 0.5309 | train: 0.5871 | Test score: 0.5356\n",
            "Epoch 473 | loss: 0.5303 | train: 0.5840 | Test score: 0.5279\n",
            "Epoch 474 | loss: 0.5282 | train: 0.5863 | Test score: 0.5285\n",
            "Epoch 475 | loss: 0.5273 | train: 0.5875 | Test score: 0.5325\n",
            "Epoch 476 | loss: 0.5291 | train: 0.5871 | Test score: 0.5316\n",
            "Epoch 477 | loss: 0.5240 | train: 0.5894 | Test score: 0.5257\n",
            "Epoch 478 | loss: 0.5315 | train: 0.5868 | Test score: 0.5381\n",
            "Epoch 479 | loss: 0.5273 | train: 0.5875 | Test score: 0.5257\n",
            "Epoch 480 | loss: 0.5269 | train: 0.5875 | Test score: 0.5301\n",
            "Epoch 481 | loss: 0.5261 | train: 0.5862 | Test score: 0.5331\n",
            "Epoch 482 | loss: 0.5281 | train: 0.5863 | Test score: 0.5220\n",
            "Epoch 483 | loss: 0.5299 | train: 0.5862 | Test score: 0.5356\n",
            "Epoch 484 | loss: 0.5243 | train: 0.5864 | Test score: 0.5294\n",
            "Epoch 485 | loss: 0.5266 | train: 0.5855 | Test score: 0.5399\n",
            "Epoch 486 | loss: 0.5269 | train: 0.5859 | Test score: 0.5298\n",
            "Epoch 487 | loss: 0.5272 | train: 0.5875 | Test score: 0.5341\n",
            "Epoch 488 | loss: 0.5275 | train: 0.5880 | Test score: 0.5375\n",
            "Epoch 489 | loss: 0.5262 | train: 0.5862 | Test score: 0.5452\n",
            "Epoch 490 | loss: 0.5245 | train: 0.5872 | Test score: 0.5338\n",
            "Epoch 491 | loss: 0.5260 | train: 0.5834 | Test score: 0.5362\n",
            "Epoch 492 | loss: 0.5287 | train: 0.5885 | Test score: 0.5368\n",
            "Epoch 493 | loss: 0.5267 | train: 0.5843 | Test score: 0.5279\n",
            "Epoch 494 | loss: 0.5255 | train: 0.5891 | Test score: 0.5387\n",
            "Epoch 495 | loss: 0.5245 | train: 0.5895 | Test score: 0.5214\n",
            "Epoch 496 | loss: 0.5261 | train: 0.5842 | Test score: 0.5449\n",
            "Epoch 497 | loss: 0.5197 | train: 0.5845 | Test score: 0.5455\n",
            "Epoch 498 | loss: 0.5260 | train: 0.5818 | Test score: 0.5344\n",
            "Epoch 499 | loss: 0.5255 | train: 0.5842 | Test score: 0.5464\n",
            "Epoch 500 | loss: 0.5256 | train: 0.5882 | Test score: 0.5433\n",
            "Epoch 501 | loss: 0.5224 | train: 0.5834 | Test score: 0.5350\n",
            "Epoch 502 | loss: 0.5228 | train: 0.5871 | Test score: 0.5405\n",
            "Epoch 503 | loss: 0.5213 | train: 0.5840 | Test score: 0.5365\n",
            "Epoch 504 | loss: 0.5241 | train: 0.5849 | Test score: 0.5205\n",
            "Epoch 505 | loss: 0.5234 | train: 0.5857 | Test score: 0.5384\n",
            "Epoch 506 | loss: 0.5206 | train: 0.5852 | Test score: 0.5372\n",
            "Epoch 507 | loss: 0.5243 | train: 0.5872 | Test score: 0.5362\n",
            "Epoch 508 | loss: 0.5227 | train: 0.5874 | Test score: 0.5409\n",
            "Epoch 509 | loss: 0.5200 | train: 0.5809 | Test score: 0.5405\n",
            "Epoch 510 | loss: 0.5231 | train: 0.5844 | Test score: 0.5344\n",
            "Epoch 511 | loss: 0.5221 | train: 0.5861 | Test score: 0.5436\n",
            "Epoch 512 | loss: 0.5186 | train: 0.5841 | Test score: 0.5365\n",
            "Epoch 513 | loss: 0.5189 | train: 0.5828 | Test score: 0.5251\n",
            "Epoch 514 | loss: 0.5171 | train: 0.5820 | Test score: 0.5372\n",
            "Epoch 515 | loss: 0.5216 | train: 0.5833 | Test score: 0.5365\n",
            "Epoch 516 | loss: 0.5169 | train: 0.5796 | Test score: 0.5298\n",
            "Epoch 517 | loss: 0.5166 | train: 0.5834 | Test score: 0.5402\n",
            "Epoch 518 | loss: 0.5186 | train: 0.5829 | Test score: 0.5387\n",
            "Epoch 519 | loss: 0.5182 | train: 0.5822 | Test score: 0.5418\n",
            "Epoch 520 | loss: 0.5171 | train: 0.5866 | Test score: 0.5338\n",
            "Epoch 521 | loss: 0.5202 | train: 0.5801 | Test score: 0.5347\n",
            "Epoch 522 | loss: 0.5222 | train: 0.5812 | Test score: 0.5285\n",
            "Epoch 523 | loss: 0.5194 | train: 0.5820 | Test score: 0.5378\n",
            "Epoch 524 | loss: 0.5219 | train: 0.5826 | Test score: 0.5405\n",
            "Epoch 525 | loss: 0.5143 | train: 0.5858 | Test score: 0.5378\n",
            "Epoch 526 | loss: 0.5179 | train: 0.5810 | Test score: 0.5393\n",
            "Epoch 527 | loss: 0.5170 | train: 0.5838 | Test score: 0.5365\n",
            "Epoch 528 | loss: 0.5143 | train: 0.5774 | Test score: 0.5384\n",
            "Epoch 529 | loss: 0.5150 | train: 0.5839 | Test score: 0.5335\n",
            "Epoch 530 | loss: 0.5160 | train: 0.5868 | Test score: 0.5442\n",
            "Epoch 531 | loss: 0.5157 | train: 0.5836 | Test score: 0.5467\n",
            "Epoch 532 | loss: 0.5154 | train: 0.5794 | Test score: 0.5331\n",
            "Epoch 533 | loss: 0.5161 | train: 0.5844 | Test score: 0.5368\n",
            "Epoch 534 | loss: 0.5186 | train: 0.5834 | Test score: 0.5405\n",
            "Epoch 535 | loss: 0.5146 | train: 0.5816 | Test score: 0.5412\n",
            "Epoch 536 | loss: 0.5141 | train: 0.5865 | Test score: 0.5335\n",
            "Epoch 537 | loss: 0.5139 | train: 0.5830 | Test score: 0.5427\n",
            "Epoch 538 | loss: 0.5114 | train: 0.5839 | Test score: 0.5378\n",
            "Epoch 539 | loss: 0.5127 | train: 0.5842 | Test score: 0.5264\n",
            "Epoch 540 | loss: 0.5138 | train: 0.5816 | Test score: 0.5350\n",
            "Epoch 541 | loss: 0.5146 | train: 0.5811 | Test score: 0.5387\n",
            "Epoch 542 | loss: 0.5124 | train: 0.5869 | Test score: 0.5381\n",
            "Epoch 543 | loss: 0.5173 | train: 0.5821 | Test score: 0.5261\n",
            "Epoch 544 | loss: 0.5161 | train: 0.5843 | Test score: 0.5375\n",
            "Epoch 545 | loss: 0.5147 | train: 0.5846 | Test score: 0.5384\n",
            "Epoch 546 | loss: 0.5132 | train: 0.5838 | Test score: 0.5307\n",
            "Epoch 547 | loss: 0.5152 | train: 0.5826 | Test score: 0.5322\n",
            "Epoch 548 | loss: 0.5146 | train: 0.5839 | Test score: 0.5353\n",
            "Epoch 549 | loss: 0.5145 | train: 0.5808 | Test score: 0.5421\n",
            "Epoch 550 | loss: 0.5151 | train: 0.5832 | Test score: 0.5301\n",
            "Epoch 551 | loss: 0.5102 | train: 0.5819 | Test score: 0.5421\n",
            "Epoch 552 | loss: 0.5095 | train: 0.5799 | Test score: 0.5338\n",
            "Epoch 553 | loss: 0.5110 | train: 0.5810 | Test score: 0.5319\n",
            "Epoch 554 | loss: 0.5106 | train: 0.5817 | Test score: 0.5276\n",
            "Epoch 555 | loss: 0.5107 | train: 0.5799 | Test score: 0.5347\n",
            "Epoch 556 | loss: 0.5140 | train: 0.5780 | Test score: 0.5279\n",
            "Epoch 557 | loss: 0.5073 | train: 0.5782 | Test score: 0.5301\n",
            "Epoch 558 | loss: 0.5115 | train: 0.5837 | Test score: 0.5362\n",
            "Epoch 559 | loss: 0.5108 | train: 0.5821 | Test score: 0.5387\n",
            "Epoch 560 | loss: 0.5118 | train: 0.5825 | Test score: 0.5402\n",
            "Epoch 561 | loss: 0.5102 | train: 0.5818 | Test score: 0.5341\n",
            "Epoch 562 | loss: 0.5074 | train: 0.5810 | Test score: 0.5301\n",
            "Epoch 563 | loss: 0.5138 | train: 0.5778 | Test score: 0.5384\n",
            "Epoch 564 | loss: 0.5097 | train: 0.5832 | Test score: 0.5298\n",
            "Epoch 565 | loss: 0.5099 | train: 0.5820 | Test score: 0.5446\n",
            "Epoch 566 | loss: 0.5078 | train: 0.5780 | Test score: 0.5415\n",
            "Epoch 567 | loss: 0.5067 | train: 0.5800 | Test score: 0.5442\n",
            "Epoch 568 | loss: 0.5056 | train: 0.5787 | Test score: 0.5375\n",
            "Epoch 569 | loss: 0.5083 | train: 0.5836 | Test score: 0.5399\n",
            "Epoch 570 | loss: 0.5113 | train: 0.5844 | Test score: 0.5421\n",
            "Epoch 571 | loss: 0.5077 | train: 0.5833 | Test score: 0.5341\n",
            "Epoch 572 | loss: 0.5080 | train: 0.5820 | Test score: 0.5368\n",
            "Epoch 573 | loss: 0.5075 | train: 0.5804 | Test score: 0.5464\n",
            "Epoch 574 | loss: 0.5057 | train: 0.5842 | Test score: 0.5381\n",
            "Epoch 575 | loss: 0.5055 | train: 0.5780 | Test score: 0.5427\n",
            "Epoch 576 | loss: 0.5066 | train: 0.5807 | Test score: 0.5372\n",
            "Epoch 577 | loss: 0.5062 | train: 0.5834 | Test score: 0.5347\n",
            "Epoch 578 | loss: 0.5087 | train: 0.5777 | Test score: 0.5341\n",
            "Epoch 579 | loss: 0.5051 | train: 0.5806 | Test score: 0.5449\n",
            "Epoch 580 | loss: 0.5090 | train: 0.5834 | Test score: 0.5424\n",
            "Epoch 581 | loss: 0.5046 | train: 0.5859 | Test score: 0.5544\n",
            "Epoch 582 | loss: 0.5074 | train: 0.5827 | Test score: 0.5328\n",
            "Epoch 583 | loss: 0.5056 | train: 0.5785 | Test score: 0.5461\n",
            "Epoch 584 | loss: 0.5053 | train: 0.5799 | Test score: 0.5393\n",
            "Epoch 585 | loss: 0.5063 | train: 0.5817 | Test score: 0.5372\n",
            "Epoch 586 | loss: 0.5047 | train: 0.5780 | Test score: 0.5381\n",
            "Epoch 587 | loss: 0.5071 | train: 0.5821 | Test score: 0.5350\n",
            "Epoch 588 | loss: 0.5073 | train: 0.5810 | Test score: 0.5424\n",
            "Epoch 589 | loss: 0.5014 | train: 0.5802 | Test score: 0.5486\n",
            "Epoch 590 | loss: 0.5043 | train: 0.5837 | Test score: 0.5436\n",
            "Epoch 591 | loss: 0.5024 | train: 0.5804 | Test score: 0.5405\n",
            "Epoch 592 | loss: 0.5062 | train: 0.5832 | Test score: 0.5526\n",
            "Epoch 593 | loss: 0.5021 | train: 0.5812 | Test score: 0.5489\n",
            "Epoch 594 | loss: 0.5007 | train: 0.5837 | Test score: 0.5418\n",
            "Epoch 595 | loss: 0.5059 | train: 0.5851 | Test score: 0.5396\n",
            "Epoch 596 | loss: 0.5027 | train: 0.5838 | Test score: 0.5381\n",
            "Epoch 597 | loss: 0.5023 | train: 0.5805 | Test score: 0.5322\n",
            "Epoch 598 | loss: 0.5007 | train: 0.5810 | Test score: 0.5381\n",
            "Epoch 599 | loss: 0.5045 | train: 0.5818 | Test score: 0.5421\n",
            "Epoch 600 | loss: 0.5007 | train: 0.5806 | Test score: 0.5365\n",
            "Epoch 601 | loss: 0.5054 | train: 0.5841 | Test score: 0.5470\n",
            "Epoch 602 | loss: 0.5001 | train: 0.5857 | Test score: 0.5399\n",
            "Epoch 603 | loss: 0.5061 | train: 0.5823 | Test score: 0.5390\n",
            "Epoch 604 | loss: 0.5009 | train: 0.5827 | Test score: 0.5415\n",
            "Epoch 605 | loss: 0.5003 | train: 0.5810 | Test score: 0.5433\n",
            "Epoch 606 | loss: 0.5027 | train: 0.5841 | Test score: 0.5341\n",
            "Epoch 607 | loss: 0.5050 | train: 0.5812 | Test score: 0.5338\n",
            "Epoch 608 | loss: 0.5023 | train: 0.5839 | Test score: 0.5442\n",
            "Epoch 609 | loss: 0.4993 | train: 0.5840 | Test score: 0.5384\n",
            "Epoch 610 | loss: 0.4990 | train: 0.5803 | Test score: 0.5304\n",
            "Epoch 611 | loss: 0.4970 | train: 0.5800 | Test score: 0.5504\n",
            "Epoch 612 | loss: 0.5021 | train: 0.5820 | Test score: 0.5322\n",
            "Epoch 613 | loss: 0.5008 | train: 0.5811 | Test score: 0.5390\n",
            "Epoch 614 | loss: 0.4991 | train: 0.5796 | Test score: 0.5430\n",
            "Epoch 615 | loss: 0.5021 | train: 0.5824 | Test score: 0.5498\n",
            "Epoch 616 | loss: 0.5009 | train: 0.5825 | Test score: 0.5396\n",
            "Epoch 617 | loss: 0.5002 | train: 0.5834 | Test score: 0.5433\n",
            "Epoch 618 | loss: 0.4951 | train: 0.5816 | Test score: 0.5365\n",
            "Epoch 619 | loss: 0.4966 | train: 0.5803 | Test score: 0.5495\n",
            "Epoch 620 | loss: 0.4997 | train: 0.5787 | Test score: 0.5282\n",
            "Epoch 621 | loss: 0.4965 | train: 0.5791 | Test score: 0.5455\n",
            "Epoch 622 | loss: 0.4979 | train: 0.5844 | Test score: 0.5455\n",
            "Epoch 623 | loss: 0.4999 | train: 0.5788 | Test score: 0.5310\n",
            "Epoch 624 | loss: 0.5020 | train: 0.5801 | Test score: 0.5220\n",
            "Epoch 625 | loss: 0.4977 | train: 0.5805 | Test score: 0.5279\n",
            "Epoch 626 | loss: 0.4975 | train: 0.5811 | Test score: 0.5390\n",
            "Epoch 627 | loss: 0.4996 | train: 0.5827 | Test score: 0.5375\n",
            "Epoch 628 | loss: 0.4951 | train: 0.5801 | Test score: 0.5470\n",
            "Epoch 629 | loss: 0.4990 | train: 0.5793 | Test score: 0.5190\n",
            "Epoch 630 | loss: 0.4980 | train: 0.5824 | Test score: 0.5418\n",
            "Epoch 631 | loss: 0.4970 | train: 0.5797 | Test score: 0.5378\n",
            "Epoch 632 | loss: 0.5023 | train: 0.5786 | Test score: 0.5486\n",
            "Epoch 633 | loss: 0.4962 | train: 0.5837 | Test score: 0.5452\n",
            "Epoch 634 | loss: 0.4991 | train: 0.5820 | Test score: 0.5513\n",
            "Epoch 635 | loss: 0.4959 | train: 0.5787 | Test score: 0.5365\n",
            "Epoch 636 | loss: 0.4948 | train: 0.5831 | Test score: 0.5476\n",
            "Epoch 637 | loss: 0.4933 | train: 0.5805 | Test score: 0.5510\n",
            "Epoch 638 | loss: 0.4925 | train: 0.5841 | Test score: 0.5467\n",
            "Epoch 639 | loss: 0.4975 | train: 0.5825 | Test score: 0.5446\n",
            "Epoch 640 | loss: 0.4968 | train: 0.5813 | Test score: 0.5452\n",
            "Epoch 641 | loss: 0.4951 | train: 0.5836 | Test score: 0.5520\n",
            "Epoch 642 | loss: 0.4958 | train: 0.5839 | Test score: 0.5439\n",
            "Epoch 643 | loss: 0.4965 | train: 0.5830 | Test score: 0.5470\n",
            "Epoch 644 | loss: 0.4939 | train: 0.5828 | Test score: 0.5581\n",
            "Epoch 645 | loss: 0.4973 | train: 0.5823 | Test score: 0.5418\n",
            "Epoch 646 | loss: 0.4959 | train: 0.5798 | Test score: 0.5418\n",
            "Epoch 647 | loss: 0.4973 | train: 0.5807 | Test score: 0.5486\n",
            "Epoch 648 | loss: 0.4963 | train: 0.5866 | Test score: 0.5439\n",
            "Epoch 649 | loss: 0.4952 | train: 0.5826 | Test score: 0.5458\n",
            "Epoch 650 | loss: 0.4926 | train: 0.5772 | Test score: 0.5338\n",
            "Epoch 651 | loss: 0.4980 | train: 0.5788 | Test score: 0.5594\n",
            "Epoch 652 | loss: 0.4948 | train: 0.5833 | Test score: 0.5504\n",
            "Epoch 653 | loss: 0.4947 | train: 0.5774 | Test score: 0.5513\n",
            "Epoch 654 | loss: 0.4954 | train: 0.5793 | Test score: 0.5375\n",
            "Epoch 655 | loss: 0.4945 | train: 0.5834 | Test score: 0.5513\n",
            "Epoch 656 | loss: 0.4907 | train: 0.5814 | Test score: 0.5495\n",
            "Epoch 657 | loss: 0.4899 | train: 0.5780 | Test score: 0.5547\n",
            "Epoch 658 | loss: 0.4904 | train: 0.5791 | Test score: 0.5557\n",
            "Epoch 659 | loss: 0.4945 | train: 0.5789 | Test score: 0.5396\n",
            "Epoch 660 | loss: 0.4933 | train: 0.5825 | Test score: 0.5375\n",
            "Epoch 661 | loss: 0.4936 | train: 0.5821 | Test score: 0.5467\n",
            "Epoch 662 | loss: 0.4920 | train: 0.5808 | Test score: 0.5597\n",
            "Epoch 663 | loss: 0.4912 | train: 0.5820 | Test score: 0.5486\n",
            "Epoch 664 | loss: 0.4876 | train: 0.5834 | Test score: 0.5384\n",
            "Epoch 665 | loss: 0.4903 | train: 0.5826 | Test score: 0.5433\n",
            "Epoch 666 | loss: 0.4932 | train: 0.5800 | Test score: 0.5513\n",
            "Epoch 667 | loss: 0.4967 | train: 0.5828 | Test score: 0.5399\n",
            "Epoch 668 | loss: 0.4911 | train: 0.5834 | Test score: 0.5532\n",
            "Epoch 669 | loss: 0.4879 | train: 0.5843 | Test score: 0.5486\n",
            "Epoch 670 | loss: 0.4931 | train: 0.5790 | Test score: 0.5452\n",
            "Epoch 671 | loss: 0.4882 | train: 0.5808 | Test score: 0.5427\n",
            "Epoch 672 | loss: 0.4862 | train: 0.5805 | Test score: 0.5433\n",
            "Epoch 673 | loss: 0.4916 | train: 0.5802 | Test score: 0.5402\n",
            "Epoch 674 | loss: 0.4915 | train: 0.5825 | Test score: 0.5402\n",
            "Epoch 675 | loss: 0.4864 | train: 0.5811 | Test score: 0.5541\n",
            "Epoch 676 | loss: 0.4901 | train: 0.5814 | Test score: 0.5402\n",
            "Epoch 677 | loss: 0.4879 | train: 0.5829 | Test score: 0.5501\n",
            "Epoch 678 | loss: 0.4922 | train: 0.5806 | Test score: 0.5461\n",
            "Epoch 679 | loss: 0.4929 | train: 0.5829 | Test score: 0.5501\n",
            "Epoch 680 | loss: 0.4861 | train: 0.5826 | Test score: 0.5492\n",
            "Epoch 681 | loss: 0.4870 | train: 0.5791 | Test score: 0.5365\n",
            "Epoch 682 | loss: 0.4912 | train: 0.5802 | Test score: 0.5541\n",
            "Epoch 683 | loss: 0.4873 | train: 0.5806 | Test score: 0.5547\n",
            "Epoch 684 | loss: 0.4905 | train: 0.5814 | Test score: 0.5489\n",
            "Epoch 685 | loss: 0.4850 | train: 0.5790 | Test score: 0.5446\n",
            "Epoch 686 | loss: 0.4898 | train: 0.5787 | Test score: 0.5637\n",
            "Epoch 687 | loss: 0.4915 | train: 0.5817 | Test score: 0.5479\n",
            "Epoch 688 | loss: 0.4883 | train: 0.5784 | Test score: 0.5535\n",
            "Epoch 689 | loss: 0.4865 | train: 0.5771 | Test score: 0.5476\n",
            "Epoch 690 | loss: 0.4880 | train: 0.5767 | Test score: 0.5541\n",
            "Epoch 691 | loss: 0.4850 | train: 0.5769 | Test score: 0.5452\n",
            "Epoch 692 | loss: 0.4865 | train: 0.5826 | Test score: 0.5483\n",
            "Epoch 693 | loss: 0.4886 | train: 0.5824 | Test score: 0.5501\n",
            "Epoch 694 | loss: 0.4865 | train: 0.5837 | Test score: 0.5449\n",
            "Epoch 695 | loss: 0.4895 | train: 0.5839 | Test score: 0.5458\n",
            "Epoch 696 | loss: 0.4858 | train: 0.5814 | Test score: 0.5393\n",
            "Epoch 697 | loss: 0.4830 | train: 0.5784 | Test score: 0.5452\n",
            "Epoch 698 | loss: 0.4872 | train: 0.5799 | Test score: 0.5516\n",
            "Epoch 699 | loss: 0.4871 | train: 0.5774 | Test score: 0.5418\n",
            "Epoch 700 | loss: 0.4855 | train: 0.5761 | Test score: 0.5495\n",
            "Epoch 701 | loss: 0.4838 | train: 0.5772 | Test score: 0.5439\n",
            "Epoch 702 | loss: 0.4860 | train: 0.5806 | Test score: 0.5473\n",
            "Epoch 703 | loss: 0.4849 | train: 0.5801 | Test score: 0.5489\n",
            "Epoch 704 | loss: 0.4849 | train: 0.5802 | Test score: 0.5393\n",
            "Epoch 705 | loss: 0.4888 | train: 0.5798 | Test score: 0.5492\n",
            "Epoch 706 | loss: 0.4884 | train: 0.5787 | Test score: 0.5427\n",
            "Epoch 707 | loss: 0.4832 | train: 0.5785 | Test score: 0.5421\n",
            "Epoch 708 | loss: 0.4847 | train: 0.5785 | Test score: 0.5458\n",
            "Epoch 709 | loss: 0.4870 | train: 0.5788 | Test score: 0.5483\n",
            "Epoch 710 | loss: 0.4831 | train: 0.5770 | Test score: 0.5446\n",
            "Epoch 711 | loss: 0.4828 | train: 0.5765 | Test score: 0.5501\n",
            "Epoch 712 | loss: 0.4814 | train: 0.5794 | Test score: 0.5461\n",
            "Epoch 713 | loss: 0.4833 | train: 0.5777 | Test score: 0.5381\n",
            "Epoch 714 | loss: 0.4835 | train: 0.5801 | Test score: 0.5436\n",
            "Epoch 715 | loss: 0.4811 | train: 0.5790 | Test score: 0.5470\n",
            "Epoch 716 | loss: 0.4852 | train: 0.5788 | Test score: 0.5396\n",
            "Epoch 717 | loss: 0.4825 | train: 0.5774 | Test score: 0.5424\n",
            "Epoch 718 | loss: 0.4809 | train: 0.5784 | Test score: 0.5504\n",
            "Epoch 719 | loss: 0.4783 | train: 0.5756 | Test score: 0.5547\n",
            "Epoch 720 | loss: 0.4823 | train: 0.5765 | Test score: 0.5430\n",
            "Epoch 721 | loss: 0.4848 | train: 0.5790 | Test score: 0.5458\n",
            "Epoch 722 | loss: 0.4814 | train: 0.5787 | Test score: 0.5261\n",
            "Epoch 723 | loss: 0.4840 | train: 0.5754 | Test score: 0.5483\n",
            "Epoch 724 | loss: 0.4861 | train: 0.5768 | Test score: 0.5504\n",
            "Epoch 725 | loss: 0.4810 | train: 0.5783 | Test score: 0.5544\n",
            "Epoch 726 | loss: 0.4835 | train: 0.5790 | Test score: 0.5575\n",
            "Epoch 727 | loss: 0.4818 | train: 0.5788 | Test score: 0.5538\n",
            "Epoch 728 | loss: 0.4801 | train: 0.5762 | Test score: 0.5587\n",
            "Epoch 729 | loss: 0.4794 | train: 0.5820 | Test score: 0.5455\n",
            "Epoch 730 | loss: 0.4791 | train: 0.5775 | Test score: 0.5591\n",
            "Epoch 731 | loss: 0.4778 | train: 0.5775 | Test score: 0.5415\n",
            "Epoch 732 | loss: 0.4835 | train: 0.5775 | Test score: 0.5507\n",
            "Epoch 733 | loss: 0.4772 | train: 0.5787 | Test score: 0.5449\n",
            "Epoch 734 | loss: 0.4775 | train: 0.5782 | Test score: 0.5430\n",
            "Epoch 735 | loss: 0.4800 | train: 0.5793 | Test score: 0.5603\n",
            "Epoch 736 | loss: 0.4785 | train: 0.5775 | Test score: 0.5479\n",
            "Epoch 737 | loss: 0.4816 | train: 0.5805 | Test score: 0.5621\n",
            "Epoch 738 | loss: 0.4820 | train: 0.5783 | Test score: 0.5510\n",
            "Epoch 739 | loss: 0.4775 | train: 0.5780 | Test score: 0.5479\n",
            "Epoch 740 | loss: 0.4795 | train: 0.5793 | Test score: 0.5479\n",
            "Epoch 741 | loss: 0.4771 | train: 0.5762 | Test score: 0.5563\n",
            "Epoch 742 | loss: 0.4805 | train: 0.5793 | Test score: 0.5569\n",
            "Epoch 743 | loss: 0.4796 | train: 0.5763 | Test score: 0.5532\n",
            "Epoch 744 | loss: 0.4769 | train: 0.5782 | Test score: 0.5560\n",
            "Epoch 745 | loss: 0.4794 | train: 0.5758 | Test score: 0.5544\n",
            "Epoch 746 | loss: 0.4784 | train: 0.5761 | Test score: 0.5553\n",
            "Epoch 747 | loss: 0.4790 | train: 0.5804 | Test score: 0.5501\n",
            "Epoch 748 | loss: 0.4778 | train: 0.5781 | Test score: 0.5458\n",
            "Epoch 749 | loss: 0.4763 | train: 0.5793 | Test score: 0.5523\n",
            "Epoch 750 | loss: 0.4764 | train: 0.5783 | Test score: 0.5587\n",
            "Epoch 751 | loss: 0.4767 | train: 0.5779 | Test score: 0.5489\n",
            "Epoch 752 | loss: 0.4761 | train: 0.5777 | Test score: 0.5591\n",
            "Epoch 753 | loss: 0.4775 | train: 0.5817 | Test score: 0.5421\n",
            "Epoch 754 | loss: 0.4782 | train: 0.5786 | Test score: 0.5486\n",
            "Epoch 755 | loss: 0.4747 | train: 0.5810 | Test score: 0.5458\n",
            "Epoch 756 | loss: 0.4723 | train: 0.5767 | Test score: 0.5486\n",
            "Epoch 757 | loss: 0.4761 | train: 0.5797 | Test score: 0.5541\n",
            "Epoch 758 | loss: 0.4782 | train: 0.5766 | Test score: 0.5328\n",
            "Epoch 759 | loss: 0.4748 | train: 0.5766 | Test score: 0.5418\n",
            "Epoch 760 | loss: 0.4785 | train: 0.5770 | Test score: 0.5372\n",
            "Epoch 761 | loss: 0.4759 | train: 0.5767 | Test score: 0.5402\n",
            "Epoch 762 | loss: 0.4777 | train: 0.5772 | Test score: 0.5483\n",
            "Epoch 763 | loss: 0.4772 | train: 0.5772 | Test score: 0.5427\n",
            "Epoch 764 | loss: 0.4747 | train: 0.5806 | Test score: 0.5335\n",
            "Epoch 765 | loss: 0.4796 | train: 0.5763 | Test score: 0.5479\n",
            "Epoch 766 | loss: 0.4768 | train: 0.5798 | Test score: 0.5399\n",
            "Epoch 767 | loss: 0.4735 | train: 0.5774 | Test score: 0.5461\n",
            "Epoch 768 | loss: 0.4752 | train: 0.5791 | Test score: 0.5412\n",
            "Epoch 769 | loss: 0.4759 | train: 0.5811 | Test score: 0.5498\n",
            "Epoch 770 | loss: 0.4758 | train: 0.5789 | Test score: 0.5421\n",
            "Epoch 771 | loss: 0.4785 | train: 0.5806 | Test score: 0.5479\n",
            "Epoch 772 | loss: 0.4783 | train: 0.5816 | Test score: 0.5476\n",
            "Epoch 773 | loss: 0.4776 | train: 0.5825 | Test score: 0.5418\n",
            "Epoch 774 | loss: 0.4784 | train: 0.5823 | Test score: 0.5464\n",
            "Epoch 775 | loss: 0.4769 | train: 0.5817 | Test score: 0.5470\n",
            "Epoch 776 | loss: 0.4740 | train: 0.5773 | Test score: 0.5381\n",
            "Epoch 777 | loss: 0.4749 | train: 0.5764 | Test score: 0.5464\n",
            "Epoch 778 | loss: 0.4752 | train: 0.5803 | Test score: 0.5405\n",
            "Epoch 779 | loss: 0.4723 | train: 0.5745 | Test score: 0.5464\n",
            "Epoch 780 | loss: 0.4705 | train: 0.5803 | Test score: 0.5591\n",
            "Epoch 781 | loss: 0.4728 | train: 0.5838 | Test score: 0.5600\n",
            "Epoch 782 | loss: 0.4707 | train: 0.5823 | Test score: 0.5526\n",
            "Epoch 783 | loss: 0.4728 | train: 0.5823 | Test score: 0.5646\n",
            "Epoch 784 | loss: 0.4733 | train: 0.5843 | Test score: 0.5553\n",
            "Epoch 785 | loss: 0.4730 | train: 0.5782 | Test score: 0.5507\n",
            "Epoch 786 | loss: 0.4742 | train: 0.5824 | Test score: 0.5560\n",
            "Epoch 787 | loss: 0.4726 | train: 0.5813 | Test score: 0.5526\n",
            "Epoch 788 | loss: 0.4746 | train: 0.5786 | Test score: 0.5464\n",
            "Epoch 789 | loss: 0.4746 | train: 0.5811 | Test score: 0.5510\n",
            "Epoch 790 | loss: 0.4738 | train: 0.5788 | Test score: 0.5495\n",
            "Epoch 791 | loss: 0.4726 | train: 0.5837 | Test score: 0.5591\n",
            "Epoch 792 | loss: 0.4734 | train: 0.5775 | Test score: 0.5624\n",
            "Epoch 793 | loss: 0.4704 | train: 0.5813 | Test score: 0.5532\n",
            "Epoch 794 | loss: 0.4771 | train: 0.5789 | Test score: 0.5529\n",
            "Epoch 795 | loss: 0.4761 | train: 0.5839 | Test score: 0.5569\n",
            "Epoch 796 | loss: 0.4682 | train: 0.5793 | Test score: 0.5501\n",
            "Epoch 797 | loss: 0.4709 | train: 0.5809 | Test score: 0.5550\n",
            "Epoch 798 | loss: 0.4696 | train: 0.5839 | Test score: 0.5550\n",
            "Epoch 799 | loss: 0.4759 | train: 0.5816 | Test score: 0.5550\n",
            "Epoch 800 | loss: 0.4713 | train: 0.5823 | Test score: 0.5529\n",
            "Epoch 801 | loss: 0.4723 | train: 0.5810 | Test score: 0.5572\n",
            "Epoch 802 | loss: 0.4689 | train: 0.5833 | Test score: 0.5513\n",
            "Epoch 803 | loss: 0.4726 | train: 0.5810 | Test score: 0.5510\n",
            "Epoch 804 | loss: 0.4677 | train: 0.5809 | Test score: 0.5461\n",
            "Epoch 805 | loss: 0.4721 | train: 0.5842 | Test score: 0.5418\n",
            "Epoch 806 | loss: 0.4691 | train: 0.5803 | Test score: 0.5510\n",
            "Epoch 807 | loss: 0.4715 | train: 0.5812 | Test score: 0.5486\n",
            "Epoch 808 | loss: 0.4712 | train: 0.5827 | Test score: 0.5492\n",
            "Epoch 809 | loss: 0.4693 | train: 0.5822 | Test score: 0.5399\n",
            "Epoch 810 | loss: 0.4714 | train: 0.5828 | Test score: 0.5606\n",
            "Epoch 811 | loss: 0.4690 | train: 0.5803 | Test score: 0.5566\n",
            "Epoch 812 | loss: 0.4715 | train: 0.5803 | Test score: 0.5547\n",
            "Epoch 813 | loss: 0.4738 | train: 0.5804 | Test score: 0.5452\n",
            "Epoch 814 | loss: 0.4702 | train: 0.5820 | Test score: 0.5495\n",
            "Epoch 815 | loss: 0.4700 | train: 0.5803 | Test score: 0.5492\n",
            "Epoch 816 | loss: 0.4703 | train: 0.5785 | Test score: 0.5523\n",
            "Epoch 817 | loss: 0.4666 | train: 0.5849 | Test score: 0.5591\n",
            "Epoch 818 | loss: 0.4700 | train: 0.5885 | Test score: 0.5510\n",
            "Epoch 819 | loss: 0.4693 | train: 0.5865 | Test score: 0.5452\n",
            "Epoch 820 | loss: 0.4693 | train: 0.5845 | Test score: 0.5615\n",
            "Epoch 821 | loss: 0.4613 | train: 0.5845 | Test score: 0.5520\n",
            "Epoch 822 | loss: 0.4652 | train: 0.5854 | Test score: 0.5553\n",
            "Epoch 823 | loss: 0.4677 | train: 0.5821 | Test score: 0.5523\n",
            "Epoch 824 | loss: 0.4706 | train: 0.5810 | Test score: 0.5483\n",
            "Epoch 825 | loss: 0.4656 | train: 0.5824 | Test score: 0.5507\n",
            "Epoch 826 | loss: 0.4646 | train: 0.5759 | Test score: 0.5550\n",
            "Epoch 827 | loss: 0.4702 | train: 0.5754 | Test score: 0.5467\n",
            "Epoch 828 | loss: 0.4662 | train: 0.5832 | Test score: 0.5529\n",
            "Epoch 829 | loss: 0.4687 | train: 0.5830 | Test score: 0.5584\n",
            "Epoch 830 | loss: 0.4705 | train: 0.5852 | Test score: 0.5538\n",
            "Epoch 831 | loss: 0.4643 | train: 0.5785 | Test score: 0.5597\n",
            "Epoch 832 | loss: 0.4685 | train: 0.5829 | Test score: 0.5547\n",
            "Epoch 833 | loss: 0.4672 | train: 0.5785 | Test score: 0.5513\n",
            "Epoch 834 | loss: 0.4626 | train: 0.5847 | Test score: 0.5575\n",
            "Epoch 835 | loss: 0.4672 | train: 0.5825 | Test score: 0.5541\n",
            "Epoch 836 | loss: 0.4684 | train: 0.5819 | Test score: 0.5587\n",
            "Epoch 837 | loss: 0.4650 | train: 0.5779 | Test score: 0.5513\n",
            "Epoch 838 | loss: 0.4658 | train: 0.5798 | Test score: 0.5516\n",
            "Epoch 839 | loss: 0.4674 | train: 0.5807 | Test score: 0.5535\n",
            "Epoch 840 | loss: 0.4659 | train: 0.5809 | Test score: 0.5452\n",
            "Epoch 841 | loss: 0.4647 | train: 0.5764 | Test score: 0.5446\n",
            "Epoch 842 | loss: 0.4677 | train: 0.5787 | Test score: 0.5455\n",
            "Epoch 843 | loss: 0.4655 | train: 0.5788 | Test score: 0.5427\n",
            "Epoch 844 | loss: 0.4657 | train: 0.5819 | Test score: 0.5541\n",
            "Epoch 845 | loss: 0.4679 | train: 0.5797 | Test score: 0.5492\n",
            "Epoch 846 | loss: 0.4664 | train: 0.5817 | Test score: 0.5538\n",
            "Epoch 847 | loss: 0.4646 | train: 0.5804 | Test score: 0.5424\n",
            "Epoch 848 | loss: 0.4652 | train: 0.5855 | Test score: 0.5560\n",
            "Epoch 849 | loss: 0.4670 | train: 0.5834 | Test score: 0.5569\n",
            "Epoch 850 | loss: 0.4659 | train: 0.5803 | Test score: 0.5470\n",
            "Epoch 851 | loss: 0.4656 | train: 0.5786 | Test score: 0.5390\n",
            "Epoch 852 | loss: 0.4608 | train: 0.5826 | Test score: 0.5393\n",
            "Epoch 853 | loss: 0.4661 | train: 0.5825 | Test score: 0.5513\n",
            "Epoch 854 | loss: 0.4660 | train: 0.5780 | Test score: 0.5507\n",
            "Epoch 855 | loss: 0.4620 | train: 0.5806 | Test score: 0.5483\n",
            "Epoch 856 | loss: 0.4661 | train: 0.5817 | Test score: 0.5399\n",
            "Epoch 857 | loss: 0.4656 | train: 0.5816 | Test score: 0.5452\n",
            "Epoch 858 | loss: 0.4655 | train: 0.5842 | Test score: 0.5439\n",
            "Epoch 859 | loss: 0.4655 | train: 0.5798 | Test score: 0.5393\n",
            "Epoch 860 | loss: 0.4635 | train: 0.5807 | Test score: 0.5501\n",
            "Epoch 861 | loss: 0.4597 | train: 0.5784 | Test score: 0.5476\n",
            "Epoch 862 | loss: 0.4628 | train: 0.5783 | Test score: 0.5412\n",
            "Epoch 863 | loss: 0.4621 | train: 0.5801 | Test score: 0.5591\n",
            "Epoch 864 | loss: 0.4659 | train: 0.5784 | Test score: 0.5455\n",
            "Epoch 865 | loss: 0.4667 | train: 0.5794 | Test score: 0.5501\n",
            "Epoch 866 | loss: 0.4627 | train: 0.5854 | Test score: 0.5513\n",
            "Epoch 867 | loss: 0.4632 | train: 0.5836 | Test score: 0.5455\n",
            "Epoch 868 | loss: 0.4618 | train: 0.5818 | Test score: 0.5486\n",
            "Epoch 869 | loss: 0.4633 | train: 0.5849 | Test score: 0.5359\n",
            "Epoch 870 | loss: 0.4627 | train: 0.5855 | Test score: 0.5547\n",
            "Epoch 871 | loss: 0.4619 | train: 0.5795 | Test score: 0.5396\n",
            "Epoch 872 | loss: 0.4617 | train: 0.5845 | Test score: 0.5381\n",
            "Epoch 873 | loss: 0.4636 | train: 0.5806 | Test score: 0.5470\n",
            "Epoch 874 | loss: 0.4622 | train: 0.5814 | Test score: 0.5430\n",
            "Epoch 875 | loss: 0.4686 | train: 0.5835 | Test score: 0.5495\n",
            "Epoch 876 | loss: 0.4615 | train: 0.5806 | Test score: 0.5446\n",
            "Epoch 877 | loss: 0.4622 | train: 0.5783 | Test score: 0.5523\n",
            "Epoch 878 | loss: 0.4628 | train: 0.5802 | Test score: 0.5316\n",
            "Epoch 879 | loss: 0.4617 | train: 0.5789 | Test score: 0.5458\n",
            "Epoch 880 | loss: 0.4608 | train: 0.5819 | Test score: 0.5492\n",
            "Epoch 881 | loss: 0.4589 | train: 0.5786 | Test score: 0.5375\n",
            "Epoch 882 | loss: 0.4622 | train: 0.5800 | Test score: 0.5449\n",
            "Epoch 883 | loss: 0.4637 | train: 0.5767 | Test score: 0.5335\n",
            "Epoch 884 | loss: 0.4585 | train: 0.5803 | Test score: 0.5486\n",
            "Epoch 885 | loss: 0.4601 | train: 0.5796 | Test score: 0.5479\n",
            "Epoch 886 | loss: 0.4594 | train: 0.5805 | Test score: 0.5507\n",
            "Epoch 887 | loss: 0.4609 | train: 0.5826 | Test score: 0.5399\n",
            "Epoch 888 | loss: 0.4608 | train: 0.5841 | Test score: 0.5449\n",
            "Epoch 889 | loss: 0.4606 | train: 0.5858 | Test score: 0.5520\n",
            "Epoch 890 | loss: 0.4580 | train: 0.5790 | Test score: 0.5541\n",
            "Epoch 891 | loss: 0.4585 | train: 0.5813 | Test score: 0.5421\n",
            "Epoch 892 | loss: 0.4583 | train: 0.5831 | Test score: 0.5347\n",
            "Epoch 893 | loss: 0.4611 | train: 0.5795 | Test score: 0.5483\n",
            "Epoch 894 | loss: 0.4589 | train: 0.5814 | Test score: 0.5507\n",
            "Epoch 895 | loss: 0.4567 | train: 0.5826 | Test score: 0.5467\n",
            "Epoch 896 | loss: 0.4591 | train: 0.5847 | Test score: 0.5479\n",
            "Epoch 897 | loss: 0.4579 | train: 0.5827 | Test score: 0.5446\n",
            "Epoch 898 | loss: 0.4590 | train: 0.5814 | Test score: 0.5455\n",
            "Epoch 899 | loss: 0.4569 | train: 0.5788 | Test score: 0.5359\n",
            "Epoch 900 | loss: 0.4584 | train: 0.5850 | Test score: 0.5442\n",
            "Epoch 901 | loss: 0.4577 | train: 0.5876 | Test score: 0.5461\n",
            "Epoch 902 | loss: 0.4630 | train: 0.5858 | Test score: 0.5310\n",
            "Epoch 903 | loss: 0.4568 | train: 0.5807 | Test score: 0.5455\n",
            "Epoch 904 | loss: 0.4617 | train: 0.5824 | Test score: 0.5393\n",
            "Epoch 905 | loss: 0.4615 | train: 0.5844 | Test score: 0.5436\n",
            "Epoch 906 | loss: 0.4570 | train: 0.5841 | Test score: 0.5523\n",
            "Epoch 907 | loss: 0.4562 | train: 0.5819 | Test score: 0.5458\n",
            "Epoch 908 | loss: 0.4627 | train: 0.5841 | Test score: 0.5464\n",
            "Epoch 909 | loss: 0.4595 | train: 0.5819 | Test score: 0.5362\n",
            "Epoch 910 | loss: 0.4581 | train: 0.5844 | Test score: 0.5390\n",
            "Epoch 911 | loss: 0.4577 | train: 0.5849 | Test score: 0.5578\n",
            "Epoch 912 | loss: 0.4560 | train: 0.5806 | Test score: 0.5532\n",
            "Epoch 913 | loss: 0.4581 | train: 0.5808 | Test score: 0.5609\n",
            "Epoch 914 | loss: 0.4524 | train: 0.5802 | Test score: 0.5439\n",
            "Epoch 915 | loss: 0.4591 | train: 0.5760 | Test score: 0.5390\n",
            "Epoch 916 | loss: 0.4545 | train: 0.5808 | Test score: 0.5600\n",
            "Epoch 917 | loss: 0.4560 | train: 0.5817 | Test score: 0.5526\n",
            "Epoch 918 | loss: 0.4580 | train: 0.5782 | Test score: 0.5473\n",
            "Epoch 919 | loss: 0.4599 | train: 0.5858 | Test score: 0.5418\n",
            "Epoch 920 | loss: 0.4582 | train: 0.5814 | Test score: 0.5538\n",
            "Epoch 921 | loss: 0.4564 | train: 0.5788 | Test score: 0.5412\n",
            "Epoch 922 | loss: 0.4589 | train: 0.5775 | Test score: 0.5486\n",
            "Epoch 923 | loss: 0.4583 | train: 0.5826 | Test score: 0.5591\n",
            "Epoch 924 | loss: 0.4540 | train: 0.5773 | Test score: 0.5458\n",
            "Epoch 925 | loss: 0.4550 | train: 0.5829 | Test score: 0.5424\n",
            "Epoch 926 | loss: 0.4579 | train: 0.5791 | Test score: 0.5606\n",
            "Epoch 927 | loss: 0.4569 | train: 0.5786 | Test score: 0.5628\n",
            "Epoch 928 | loss: 0.4565 | train: 0.5815 | Test score: 0.5606\n",
            "Epoch 929 | loss: 0.4547 | train: 0.5764 | Test score: 0.5584\n",
            "Epoch 930 | loss: 0.4526 | train: 0.5807 | Test score: 0.5495\n",
            "Epoch 931 | loss: 0.4586 | train: 0.5821 | Test score: 0.5581\n",
            "Epoch 932 | loss: 0.4522 | train: 0.5847 | Test score: 0.5563\n",
            "Epoch 933 | loss: 0.4556 | train: 0.5822 | Test score: 0.5566\n",
            "Epoch 934 | loss: 0.4586 | train: 0.5824 | Test score: 0.5557\n",
            "Epoch 935 | loss: 0.4580 | train: 0.5824 | Test score: 0.5498\n",
            "Epoch 936 | loss: 0.4538 | train: 0.5796 | Test score: 0.5541\n",
            "Epoch 937 | loss: 0.4528 | train: 0.5816 | Test score: 0.5612\n",
            "Epoch 938 | loss: 0.4565 | train: 0.5817 | Test score: 0.5581\n",
            "Epoch 939 | loss: 0.4531 | train: 0.5774 | Test score: 0.5409\n",
            "Epoch 940 | loss: 0.4554 | train: 0.5725 | Test score: 0.5646\n",
            "Epoch 941 | loss: 0.4566 | train: 0.5784 | Test score: 0.5535\n",
            "Epoch 942 | loss: 0.4494 | train: 0.5809 | Test score: 0.5495\n",
            "Epoch 943 | loss: 0.4542 | train: 0.5813 | Test score: 0.5615\n",
            "Epoch 944 | loss: 0.4554 | train: 0.5825 | Test score: 0.5575\n",
            "Epoch 945 | loss: 0.4534 | train: 0.5839 | Test score: 0.5594\n",
            "Epoch 946 | loss: 0.4546 | train: 0.5781 | Test score: 0.5476\n",
            "Epoch 947 | loss: 0.4596 | train: 0.5796 | Test score: 0.5427\n",
            "Epoch 948 | loss: 0.4565 | train: 0.5767 | Test score: 0.5375\n",
            "Epoch 949 | loss: 0.4545 | train: 0.5809 | Test score: 0.5433\n",
            "Epoch 950 | loss: 0.4562 | train: 0.5785 | Test score: 0.5455\n",
            "Epoch 951 | loss: 0.4576 | train: 0.5841 | Test score: 0.5569\n",
            "Epoch 952 | loss: 0.4490 | train: 0.5792 | Test score: 0.5495\n",
            "Epoch 953 | loss: 0.4547 | train: 0.5796 | Test score: 0.5665\n",
            "Epoch 954 | loss: 0.4484 | train: 0.5777 | Test score: 0.5634\n",
            "Epoch 955 | loss: 0.4484 | train: 0.5768 | Test score: 0.5557\n",
            "Epoch 956 | loss: 0.4531 | train: 0.5811 | Test score: 0.5541\n",
            "Epoch 957 | loss: 0.4533 | train: 0.5796 | Test score: 0.5553\n",
            "Epoch 958 | loss: 0.4525 | train: 0.5801 | Test score: 0.5473\n",
            "Epoch 959 | loss: 0.4517 | train: 0.5810 | Test score: 0.5495\n",
            "Epoch 960 | loss: 0.4504 | train: 0.5763 | Test score: 0.5402\n",
            "Epoch 961 | loss: 0.4526 | train: 0.5792 | Test score: 0.5492\n",
            "Epoch 962 | loss: 0.4521 | train: 0.5747 | Test score: 0.5479\n",
            "Epoch 963 | loss: 0.4548 | train: 0.5755 | Test score: 0.5402\n",
            "Epoch 964 | loss: 0.4524 | train: 0.5832 | Test score: 0.5319\n",
            "Epoch 965 | loss: 0.4520 | train: 0.5812 | Test score: 0.5486\n",
            "Epoch 966 | loss: 0.4529 | train: 0.5814 | Test score: 0.5464\n",
            "Epoch 967 | loss: 0.4485 | train: 0.5847 | Test score: 0.5541\n",
            "Epoch 968 | loss: 0.4536 | train: 0.5764 | Test score: 0.5529\n",
            "Epoch 969 | loss: 0.4541 | train: 0.5788 | Test score: 0.5501\n",
            "Epoch 970 | loss: 0.4535 | train: 0.5814 | Test score: 0.5467\n",
            "Epoch 971 | loss: 0.4530 | train: 0.5795 | Test score: 0.5526\n",
            "Epoch 972 | loss: 0.4526 | train: 0.5766 | Test score: 0.5557\n",
            "Epoch 973 | loss: 0.4537 | train: 0.5815 | Test score: 0.5516\n",
            "Epoch 974 | loss: 0.4538 | train: 0.5778 | Test score: 0.5489\n",
            "Epoch 975 | loss: 0.4490 | train: 0.5792 | Test score: 0.5439\n",
            "Epoch 976 | loss: 0.4515 | train: 0.5807 | Test score: 0.5504\n",
            "Epoch 977 | loss: 0.4530 | train: 0.5804 | Test score: 0.5476\n",
            "Epoch 978 | loss: 0.4573 | train: 0.5812 | Test score: 0.5516\n",
            "Epoch 979 | loss: 0.4530 | train: 0.5805 | Test score: 0.5458\n",
            "Epoch 980 | loss: 0.4536 | train: 0.5814 | Test score: 0.5467\n",
            "Epoch 981 | loss: 0.4503 | train: 0.5825 | Test score: 0.5575\n",
            "Epoch 982 | loss: 0.4486 | train: 0.5789 | Test score: 0.5631\n",
            "Epoch 983 | loss: 0.4505 | train: 0.5759 | Test score: 0.5544\n",
            "Epoch 984 | loss: 0.4494 | train: 0.5749 | Test score: 0.5430\n",
            "Epoch 985 | loss: 0.4506 | train: 0.5843 | Test score: 0.5470\n",
            "Epoch 986 | loss: 0.4502 | train: 0.5744 | Test score: 0.5507\n",
            "Epoch 987 | loss: 0.4510 | train: 0.5781 | Test score: 0.5581\n",
            "Epoch 988 | loss: 0.4532 | train: 0.5831 | Test score: 0.5427\n",
            "Epoch 989 | loss: 0.4508 | train: 0.5811 | Test score: 0.5372\n",
            "Epoch 990 | loss: 0.4493 | train: 0.5798 | Test score: 0.5470\n",
            "Epoch 991 | loss: 0.4490 | train: 0.5821 | Test score: 0.5557\n",
            "Epoch 992 | loss: 0.4469 | train: 0.5827 | Test score: 0.5479\n",
            "Epoch 993 | loss: 0.4463 | train: 0.5777 | Test score: 0.5455\n",
            "Epoch 994 | loss: 0.4525 | train: 0.5796 | Test score: 0.5430\n",
            "Epoch 995 | loss: 0.4483 | train: 0.5807 | Test score: 0.5368\n",
            "Epoch 996 | loss: 0.4472 | train: 0.5800 | Test score: 0.5387\n",
            "Epoch 997 | loss: 0.4507 | train: 0.5823 | Test score: 0.5328\n",
            "Epoch 998 | loss: 0.4530 | train: 0.5828 | Test score: 0.5409\n",
            "Epoch 999 | loss: 0.4488 | train: 0.5817 | Test score: 0.5486\n",
            "Epoch 1000 | loss: 0.4492 | train: 0.5780 | Test score: 0.5461\n",
            "Epoch 1001 | loss: 0.4517 | train: 0.5826 | Test score: 0.5581\n",
            "Epoch 1002 | loss: 0.4468 | train: 0.5816 | Test score: 0.5557\n",
            "Epoch 1003 | loss: 0.4489 | train: 0.5784 | Test score: 0.5473\n",
            "Epoch 1004 | loss: 0.4473 | train: 0.5812 | Test score: 0.5479\n",
            "Epoch 1005 | loss: 0.4490 | train: 0.5790 | Test score: 0.5415\n",
            "Epoch 1006 | loss: 0.4480 | train: 0.5783 | Test score: 0.5591\n",
            "Epoch 1007 | loss: 0.4527 | train: 0.5776 | Test score: 0.5483\n",
            "Epoch 1008 | loss: 0.4447 | train: 0.5823 | Test score: 0.5424\n",
            "Epoch 1009 | loss: 0.4484 | train: 0.5785 | Test score: 0.5449\n",
            "Epoch 1010 | loss: 0.4502 | train: 0.5785 | Test score: 0.5523\n",
            "Epoch 1011 | loss: 0.4459 | train: 0.5795 | Test score: 0.5461\n",
            "Epoch 1012 | loss: 0.4474 | train: 0.5843 | Test score: 0.5449\n",
            "Epoch 1013 | loss: 0.4463 | train: 0.5778 | Test score: 0.5510\n",
            "Epoch 1014 | loss: 0.4490 | train: 0.5828 | Test score: 0.5553\n",
            "Epoch 1015 | loss: 0.4468 | train: 0.5791 | Test score: 0.5464\n",
            "Epoch 1016 | loss: 0.4480 | train: 0.5803 | Test score: 0.5415\n",
            "Epoch 1017 | loss: 0.4509 | train: 0.5812 | Test score: 0.5436\n",
            "Epoch 1018 | loss: 0.4466 | train: 0.5793 | Test score: 0.5498\n",
            "Epoch 1019 | loss: 0.4486 | train: 0.5780 | Test score: 0.5378\n",
            "Epoch 1020 | loss: 0.4480 | train: 0.5832 | Test score: 0.5365\n",
            "Epoch 1021 | loss: 0.4474 | train: 0.5835 | Test score: 0.5578\n",
            "Epoch 1022 | loss: 0.4448 | train: 0.5827 | Test score: 0.5547\n",
            "Epoch 1023 | loss: 0.4486 | train: 0.5819 | Test score: 0.5319\n",
            "Epoch 1024 | loss: 0.4419 | train: 0.5838 | Test score: 0.5532\n",
            "Epoch 1025 | loss: 0.4466 | train: 0.5814 | Test score: 0.5476\n",
            "Epoch 1026 | loss: 0.4465 | train: 0.5792 | Test score: 0.5473\n",
            "Epoch 1027 | loss: 0.4449 | train: 0.5811 | Test score: 0.5436\n",
            "Epoch 1028 | loss: 0.4454 | train: 0.5773 | Test score: 0.5319\n",
            "Epoch 1029 | loss: 0.4397 | train: 0.5751 | Test score: 0.5452\n",
            "Epoch 1030 | loss: 0.4417 | train: 0.5842 | Test score: 0.5396\n",
            "Epoch 1031 | loss: 0.4446 | train: 0.5798 | Test score: 0.5322\n",
            "Epoch 1032 | loss: 0.4448 | train: 0.5811 | Test score: 0.5446\n",
            "Epoch 1033 | loss: 0.4462 | train: 0.5826 | Test score: 0.5479\n",
            "Epoch 1034 | loss: 0.4482 | train: 0.5825 | Test score: 0.5446\n",
            "Epoch 1035 | loss: 0.4459 | train: 0.5841 | Test score: 0.5473\n",
            "Epoch 1036 | loss: 0.4477 | train: 0.5819 | Test score: 0.5495\n",
            "Epoch 1037 | loss: 0.4455 | train: 0.5812 | Test score: 0.5421\n",
            "Epoch 1038 | loss: 0.4433 | train: 0.5808 | Test score: 0.5461\n",
            "Epoch 1039 | loss: 0.4441 | train: 0.5818 | Test score: 0.5427\n",
            "Epoch 1040 | loss: 0.4451 | train: 0.5803 | Test score: 0.5510\n",
            "Epoch 1041 | loss: 0.4423 | train: 0.5819 | Test score: 0.5513\n",
            "Epoch 1042 | loss: 0.4471 | train: 0.5789 | Test score: 0.5563\n",
            "Epoch 1043 | loss: 0.4443 | train: 0.5856 | Test score: 0.5584\n",
            "Epoch 1044 | loss: 0.4426 | train: 0.5795 | Test score: 0.5510\n",
            "Epoch 1045 | loss: 0.4421 | train: 0.5837 | Test score: 0.5501\n",
            "Epoch 1046 | loss: 0.4459 | train: 0.5774 | Test score: 0.5387\n",
            "Epoch 1047 | loss: 0.4397 | train: 0.5794 | Test score: 0.5362\n",
            "Epoch 1048 | loss: 0.4449 | train: 0.5807 | Test score: 0.5489\n",
            "Epoch 1049 | loss: 0.4460 | train: 0.5804 | Test score: 0.5507\n",
            "Epoch 1050 | loss: 0.4413 | train: 0.5786 | Test score: 0.5498\n",
            "Epoch 1051 | loss: 0.4407 | train: 0.5778 | Test score: 0.5544\n",
            "Epoch 1052 | loss: 0.4448 | train: 0.5789 | Test score: 0.5424\n",
            "Epoch 1053 | loss: 0.4465 | train: 0.5788 | Test score: 0.5510\n",
            "Epoch 1054 | loss: 0.4416 | train: 0.5803 | Test score: 0.5495\n",
            "Epoch 1055 | loss: 0.4476 | train: 0.5787 | Test score: 0.5547\n",
            "Epoch 1056 | loss: 0.4449 | train: 0.5805 | Test score: 0.5535\n",
            "Epoch 1057 | loss: 0.4448 | train: 0.5787 | Test score: 0.5550\n",
            "Epoch 1058 | loss: 0.4444 | train: 0.5770 | Test score: 0.5430\n",
            "Epoch 1059 | loss: 0.4443 | train: 0.5793 | Test score: 0.5541\n",
            "Epoch 1060 | loss: 0.4450 | train: 0.5803 | Test score: 0.5566\n",
            "Epoch 1061 | loss: 0.4396 | train: 0.5804 | Test score: 0.5523\n",
            "Epoch 1062 | loss: 0.4439 | train: 0.5819 | Test score: 0.5526\n",
            "Epoch 1063 | loss: 0.4426 | train: 0.5777 | Test score: 0.5476\n",
            "Epoch 1064 | loss: 0.4408 | train: 0.5798 | Test score: 0.5449\n",
            "Epoch 1065 | loss: 0.4422 | train: 0.5781 | Test score: 0.5498\n",
            "Epoch 1066 | loss: 0.4389 | train: 0.5776 | Test score: 0.5430\n",
            "Epoch 1067 | loss: 0.4444 | train: 0.5803 | Test score: 0.5304\n",
            "Epoch 1068 | loss: 0.4442 | train: 0.5810 | Test score: 0.5532\n",
            "Epoch 1069 | loss: 0.4429 | train: 0.5757 | Test score: 0.5566\n",
            "Epoch 1070 | loss: 0.4447 | train: 0.5762 | Test score: 0.5594\n",
            "Epoch 1071 | loss: 0.4372 | train: 0.5788 | Test score: 0.5538\n",
            "Epoch 1072 | loss: 0.4385 | train: 0.5783 | Test score: 0.5510\n",
            "Epoch 1073 | loss: 0.4402 | train: 0.5796 | Test score: 0.5532\n",
            "Epoch 1074 | loss: 0.4398 | train: 0.5815 | Test score: 0.5526\n",
            "Epoch 1075 | loss: 0.4400 | train: 0.5836 | Test score: 0.5442\n",
            "Epoch 1076 | loss: 0.4429 | train: 0.5831 | Test score: 0.5498\n",
            "Epoch 1077 | loss: 0.4389 | train: 0.5788 | Test score: 0.5424\n",
            "Epoch 1078 | loss: 0.4425 | train: 0.5787 | Test score: 0.5550\n",
            "Epoch 1079 | loss: 0.4374 | train: 0.5847 | Test score: 0.5415\n",
            "Epoch 1080 | loss: 0.4422 | train: 0.5847 | Test score: 0.5439\n",
            "Epoch 1081 | loss: 0.4414 | train: 0.5788 | Test score: 0.5476\n",
            "Epoch 1082 | loss: 0.4391 | train: 0.5796 | Test score: 0.5501\n",
            "Epoch 1083 | loss: 0.4419 | train: 0.5809 | Test score: 0.5421\n",
            "Epoch 1084 | loss: 0.4491 | train: 0.5783 | Test score: 0.5541\n",
            "Epoch 1085 | loss: 0.4425 | train: 0.5797 | Test score: 0.5442\n",
            "Epoch 1086 | loss: 0.4382 | train: 0.5808 | Test score: 0.5563\n",
            "Epoch 1087 | loss: 0.4361 | train: 0.5817 | Test score: 0.5544\n",
            "Epoch 1088 | loss: 0.4403 | train: 0.5784 | Test score: 0.5473\n",
            "Epoch 1089 | loss: 0.4416 | train: 0.5798 | Test score: 0.5476\n",
            "Epoch 1090 | loss: 0.4412 | train: 0.5784 | Test score: 0.5409\n",
            "Epoch 1091 | loss: 0.4446 | train: 0.5825 | Test score: 0.5455\n",
            "Epoch 1092 | loss: 0.4405 | train: 0.5809 | Test score: 0.5483\n",
            "Epoch 1093 | loss: 0.4392 | train: 0.5794 | Test score: 0.5507\n",
            "Epoch 1094 | loss: 0.4411 | train: 0.5792 | Test score: 0.5526\n",
            "Epoch 1095 | loss: 0.4369 | train: 0.5765 | Test score: 0.5378\n",
            "Epoch 1096 | loss: 0.4407 | train: 0.5773 | Test score: 0.5467\n",
            "Epoch 1097 | loss: 0.4389 | train: 0.5803 | Test score: 0.5347\n",
            "Epoch 1098 | loss: 0.4398 | train: 0.5788 | Test score: 0.5442\n",
            "Epoch 1099 | loss: 0.4405 | train: 0.5813 | Test score: 0.5446\n",
            "Epoch 1100 | loss: 0.4379 | train: 0.5787 | Test score: 0.5350\n",
            "Epoch 1101 | loss: 0.4412 | train: 0.5792 | Test score: 0.5393\n",
            "Epoch 1102 | loss: 0.4378 | train: 0.5804 | Test score: 0.5550\n",
            "Epoch 1103 | loss: 0.4438 | train: 0.5821 | Test score: 0.5430\n",
            "Epoch 1104 | loss: 0.4387 | train: 0.5829 | Test score: 0.5421\n",
            "Epoch 1105 | loss: 0.4400 | train: 0.5769 | Test score: 0.5458\n",
            "Epoch 1106 | loss: 0.4407 | train: 0.5780 | Test score: 0.5421\n",
            "Epoch 1107 | loss: 0.4375 | train: 0.5791 | Test score: 0.5344\n",
            "Epoch 1108 | loss: 0.4361 | train: 0.5791 | Test score: 0.5560\n",
            "Epoch 1109 | loss: 0.4408 | train: 0.5787 | Test score: 0.5433\n",
            "Epoch 1110 | loss: 0.4383 | train: 0.5747 | Test score: 0.5473\n",
            "Epoch 1111 | loss: 0.4367 | train: 0.5739 | Test score: 0.5529\n",
            "Epoch 1112 | loss: 0.4377 | train: 0.5766 | Test score: 0.5362\n",
            "Epoch 1113 | loss: 0.4439 | train: 0.5724 | Test score: 0.5304\n",
            "Epoch 1114 | loss: 0.4348 | train: 0.5805 | Test score: 0.5384\n",
            "Epoch 1115 | loss: 0.4408 | train: 0.5799 | Test score: 0.5409\n",
            "Epoch 1116 | loss: 0.4399 | train: 0.5761 | Test score: 0.5356\n",
            "Epoch 1117 | loss: 0.4430 | train: 0.5766 | Test score: 0.5347\n",
            "Epoch 1118 | loss: 0.4388 | train: 0.5794 | Test score: 0.5362\n",
            "Epoch 1119 | loss: 0.4401 | train: 0.5771 | Test score: 0.5384\n",
            "Epoch 1120 | loss: 0.4391 | train: 0.5769 | Test score: 0.5455\n",
            "Epoch 1121 | loss: 0.4382 | train: 0.5798 | Test score: 0.5449\n",
            "Epoch 1122 | loss: 0.4357 | train: 0.5785 | Test score: 0.5553\n",
            "Epoch 1123 | loss: 0.4353 | train: 0.5754 | Test score: 0.5347\n",
            "Epoch 1124 | loss: 0.4406 | train: 0.5767 | Test score: 0.5396\n",
            "Epoch 1125 | loss: 0.4360 | train: 0.5789 | Test score: 0.5458\n",
            "Epoch 1126 | loss: 0.4405 | train: 0.5837 | Test score: 0.5486\n",
            "Epoch 1127 | loss: 0.4358 | train: 0.5806 | Test score: 0.5535\n",
            "Epoch 1128 | loss: 0.4365 | train: 0.5778 | Test score: 0.5473\n",
            "Epoch 1129 | loss: 0.4374 | train: 0.5777 | Test score: 0.5479\n",
            "Epoch 1130 | loss: 0.4354 | train: 0.5758 | Test score: 0.5486\n",
            "Epoch 1131 | loss: 0.4305 | train: 0.5794 | Test score: 0.5470\n",
            "Epoch 1132 | loss: 0.4377 | train: 0.5837 | Test score: 0.5449\n",
            "Epoch 1133 | loss: 0.4365 | train: 0.5765 | Test score: 0.5572\n",
            "Epoch 1134 | loss: 0.4351 | train: 0.5779 | Test score: 0.5470\n",
            "Epoch 1135 | loss: 0.4375 | train: 0.5799 | Test score: 0.5390\n",
            "Epoch 1136 | loss: 0.4341 | train: 0.5794 | Test score: 0.5572\n",
            "Epoch 1137 | loss: 0.4387 | train: 0.5773 | Test score: 0.5390\n",
            "Epoch 1138 | loss: 0.4351 | train: 0.5741 | Test score: 0.5430\n",
            "Epoch 1139 | loss: 0.4381 | train: 0.5768 | Test score: 0.5347\n",
            "Epoch 1140 | loss: 0.4368 | train: 0.5778 | Test score: 0.5492\n",
            "Epoch 1141 | loss: 0.4390 | train: 0.5801 | Test score: 0.5402\n",
            "Epoch 1142 | loss: 0.4342 | train: 0.5789 | Test score: 0.5476\n",
            "Epoch 1143 | loss: 0.4330 | train: 0.5767 | Test score: 0.5436\n",
            "Epoch 1144 | loss: 0.4346 | train: 0.5761 | Test score: 0.5421\n",
            "Epoch 1145 | loss: 0.4377 | train: 0.5802 | Test score: 0.5572\n",
            "Epoch 1146 | loss: 0.4295 | train: 0.5803 | Test score: 0.5415\n",
            "Epoch 1147 | loss: 0.4349 | train: 0.5798 | Test score: 0.5442\n",
            "Epoch 1148 | loss: 0.4378 | train: 0.5786 | Test score: 0.5523\n",
            "Epoch 1149 | loss: 0.4284 | train: 0.5795 | Test score: 0.5501\n",
            "Epoch 1150 | loss: 0.4353 | train: 0.5801 | Test score: 0.5520\n",
            "Epoch 1151 | loss: 0.4390 | train: 0.5816 | Test score: 0.5399\n",
            "Epoch 1152 | loss: 0.4378 | train: 0.5812 | Test score: 0.5529\n",
            "Epoch 1153 | loss: 0.4352 | train: 0.5773 | Test score: 0.5523\n",
            "Epoch 1154 | loss: 0.4382 | train: 0.5779 | Test score: 0.5415\n",
            "Epoch 1155 | loss: 0.4346 | train: 0.5810 | Test score: 0.5449\n",
            "Epoch 1156 | loss: 0.4338 | train: 0.5802 | Test score: 0.5572\n",
            "Epoch 1157 | loss: 0.4359 | train: 0.5778 | Test score: 0.5486\n",
            "Epoch 1158 | loss: 0.4361 | train: 0.5791 | Test score: 0.5452\n",
            "Epoch 1159 | loss: 0.4347 | train: 0.5788 | Test score: 0.5461\n",
            "Epoch 1160 | loss: 0.4378 | train: 0.5800 | Test score: 0.5427\n",
            "Epoch 1161 | loss: 0.4338 | train: 0.5783 | Test score: 0.5350\n",
            "Epoch 1162 | loss: 0.4367 | train: 0.5777 | Test score: 0.5412\n",
            "Epoch 1163 | loss: 0.4336 | train: 0.5793 | Test score: 0.5415\n",
            "Epoch 1164 | loss: 0.4350 | train: 0.5743 | Test score: 0.5436\n",
            "Epoch 1165 | loss: 0.4317 | train: 0.5828 | Test score: 0.5553\n",
            "Epoch 1166 | loss: 0.4356 | train: 0.5777 | Test score: 0.5461\n",
            "Epoch 1167 | loss: 0.4350 | train: 0.5779 | Test score: 0.5368\n",
            "Epoch 1168 | loss: 0.4369 | train: 0.5774 | Test score: 0.5486\n",
            "Epoch 1169 | loss: 0.4323 | train: 0.5784 | Test score: 0.5470\n",
            "Epoch 1170 | loss: 0.4335 | train: 0.5776 | Test score: 0.5381\n",
            "Epoch 1171 | loss: 0.4354 | train: 0.5754 | Test score: 0.5421\n",
            "Epoch 1172 | loss: 0.4311 | train: 0.5812 | Test score: 0.5424\n",
            "Epoch 1173 | loss: 0.4385 | train: 0.5796 | Test score: 0.5452\n",
            "Epoch 1174 | loss: 0.4333 | train: 0.5760 | Test score: 0.5461\n",
            "Epoch 1175 | loss: 0.4344 | train: 0.5775 | Test score: 0.5433\n",
            "Epoch 1176 | loss: 0.4326 | train: 0.5781 | Test score: 0.5390\n",
            "Epoch 1177 | loss: 0.4320 | train: 0.5766 | Test score: 0.5427\n",
            "Epoch 1178 | loss: 0.4360 | train: 0.5767 | Test score: 0.5402\n",
            "Epoch 1179 | loss: 0.4298 | train: 0.5800 | Test score: 0.5510\n",
            "Epoch 1180 | loss: 0.4357 | train: 0.5739 | Test score: 0.5421\n",
            "Epoch 1181 | loss: 0.4366 | train: 0.5774 | Test score: 0.5446\n",
            "Epoch 1182 | loss: 0.4389 | train: 0.5743 | Test score: 0.5409\n",
            "Epoch 1183 | loss: 0.4374 | train: 0.5748 | Test score: 0.5486\n",
            "Epoch 1184 | loss: 0.4347 | train: 0.5805 | Test score: 0.5399\n",
            "Epoch 1185 | loss: 0.4342 | train: 0.5776 | Test score: 0.5433\n",
            "Epoch 1186 | loss: 0.4359 | train: 0.5777 | Test score: 0.5486\n",
            "Epoch 1187 | loss: 0.4335 | train: 0.5768 | Test score: 0.5350\n",
            "Epoch 1188 | loss: 0.4358 | train: 0.5761 | Test score: 0.5399\n",
            "Epoch 1189 | loss: 0.4355 | train: 0.5775 | Test score: 0.5402\n",
            "Epoch 1190 | loss: 0.4299 | train: 0.5774 | Test score: 0.5399\n",
            "Epoch 1191 | loss: 0.4340 | train: 0.5813 | Test score: 0.5458\n",
            "Epoch 1192 | loss: 0.4351 | train: 0.5828 | Test score: 0.5409\n",
            "Epoch 1193 | loss: 0.4321 | train: 0.5766 | Test score: 0.5378\n",
            "Epoch 1194 | loss: 0.4305 | train: 0.5780 | Test score: 0.5409\n",
            "Epoch 1195 | loss: 0.4286 | train: 0.5782 | Test score: 0.5381\n",
            "Epoch 1196 | loss: 0.4326 | train: 0.5743 | Test score: 0.5436\n",
            "Epoch 1197 | loss: 0.4312 | train: 0.5775 | Test score: 0.5470\n",
            "Epoch 1198 | loss: 0.4360 | train: 0.5782 | Test score: 0.5359\n",
            "Epoch 1199 | loss: 0.4383 | train: 0.5776 | Test score: 0.5390\n",
            "Epoch 1200 | loss: 0.4306 | train: 0.5763 | Test score: 0.5470\n",
            "Epoch 1201 | loss: 0.4305 | train: 0.5766 | Test score: 0.5294\n",
            "Epoch 1202 | loss: 0.4346 | train: 0.5777 | Test score: 0.5393\n",
            "Epoch 1203 | loss: 0.4333 | train: 0.5783 | Test score: 0.5399\n",
            "Epoch 1204 | loss: 0.4308 | train: 0.5757 | Test score: 0.5405\n",
            "Epoch 1205 | loss: 0.4310 | train: 0.5752 | Test score: 0.5446\n",
            "Epoch 1206 | loss: 0.4268 | train: 0.5802 | Test score: 0.5430\n",
            "Epoch 1207 | loss: 0.4319 | train: 0.5768 | Test score: 0.5421\n",
            "Epoch 1208 | loss: 0.4269 | train: 0.5798 | Test score: 0.5368\n",
            "Epoch 1209 | loss: 0.4291 | train: 0.5741 | Test score: 0.5282\n",
            "Epoch 1210 | loss: 0.4342 | train: 0.5788 | Test score: 0.5446\n",
            "Epoch 1211 | loss: 0.4324 | train: 0.5779 | Test score: 0.5430\n",
            "Epoch 1212 | loss: 0.4286 | train: 0.5773 | Test score: 0.5430\n",
            "Epoch 1213 | loss: 0.4305 | train: 0.5755 | Test score: 0.5415\n",
            "Epoch 1214 | loss: 0.4312 | train: 0.5772 | Test score: 0.5449\n",
            "Epoch 1215 | loss: 0.4335 | train: 0.5769 | Test score: 0.5427\n",
            "Epoch 1216 | loss: 0.4325 | train: 0.5771 | Test score: 0.5572\n",
            "Epoch 1217 | loss: 0.4298 | train: 0.5820 | Test score: 0.5516\n",
            "Epoch 1218 | loss: 0.4306 | train: 0.5801 | Test score: 0.5464\n",
            "Epoch 1219 | loss: 0.4278 | train: 0.5770 | Test score: 0.5412\n",
            "Epoch 1220 | loss: 0.4322 | train: 0.5779 | Test score: 0.5396\n",
            "Epoch 1221 | loss: 0.4288 | train: 0.5798 | Test score: 0.5430\n",
            "Epoch 1222 | loss: 0.4289 | train: 0.5748 | Test score: 0.5412\n",
            "Epoch 1223 | loss: 0.4291 | train: 0.5795 | Test score: 0.5396\n",
            "Epoch 1224 | loss: 0.4300 | train: 0.5759 | Test score: 0.5430\n",
            "Epoch 1225 | loss: 0.4280 | train: 0.5781 | Test score: 0.5291\n",
            "Epoch 1226 | loss: 0.4255 | train: 0.5801 | Test score: 0.5316\n",
            "Epoch 1227 | loss: 0.4303 | train: 0.5808 | Test score: 0.5291\n",
            "Epoch 1228 | loss: 0.4283 | train: 0.5780 | Test score: 0.5399\n",
            "Epoch 1229 | loss: 0.4283 | train: 0.5804 | Test score: 0.5433\n",
            "Epoch 1230 | loss: 0.4284 | train: 0.5764 | Test score: 0.5402\n",
            "Epoch 1231 | loss: 0.4331 | train: 0.5780 | Test score: 0.5390\n",
            "Epoch 1232 | loss: 0.4290 | train: 0.5759 | Test score: 0.5381\n",
            "Epoch 1233 | loss: 0.4280 | train: 0.5784 | Test score: 0.5449\n",
            "Epoch 1234 | loss: 0.4299 | train: 0.5780 | Test score: 0.5452\n",
            "Epoch 1235 | loss: 0.4301 | train: 0.5766 | Test score: 0.5384\n",
            "Epoch 1236 | loss: 0.4295 | train: 0.5781 | Test score: 0.5461\n",
            "Epoch 1237 | loss: 0.4296 | train: 0.5765 | Test score: 0.5421\n",
            "Epoch 1238 | loss: 0.4306 | train: 0.5782 | Test score: 0.5547\n",
            "Epoch 1239 | loss: 0.4261 | train: 0.5766 | Test score: 0.5439\n",
            "Epoch 1240 | loss: 0.4264 | train: 0.5759 | Test score: 0.5541\n",
            "Epoch 1241 | loss: 0.4277 | train: 0.5740 | Test score: 0.5433\n",
            "Epoch 1242 | loss: 0.4290 | train: 0.5755 | Test score: 0.5362\n",
            "Epoch 1243 | loss: 0.4309 | train: 0.5760 | Test score: 0.5461\n",
            "Epoch 1244 | loss: 0.4337 | train: 0.5768 | Test score: 0.5421\n",
            "Epoch 1245 | loss: 0.4261 | train: 0.5757 | Test score: 0.5325\n",
            "Epoch 1246 | loss: 0.4272 | train: 0.5783 | Test score: 0.5421\n",
            "Epoch 1247 | loss: 0.4290 | train: 0.5791 | Test score: 0.5446\n",
            "Epoch 1248 | loss: 0.4263 | train: 0.5799 | Test score: 0.5427\n",
            "Epoch 1249 | loss: 0.4254 | train: 0.5753 | Test score: 0.5430\n",
            "Epoch 1250 | loss: 0.4270 | train: 0.5781 | Test score: 0.5381\n",
            "Epoch 1251 | loss: 0.4291 | train: 0.5753 | Test score: 0.5470\n",
            "Epoch 1252 | loss: 0.4290 | train: 0.5799 | Test score: 0.5452\n",
            "Epoch 1253 | loss: 0.4290 | train: 0.5791 | Test score: 0.5452\n",
            "Epoch 1254 | loss: 0.4288 | train: 0.5774 | Test score: 0.5381\n",
            "Epoch 1255 | loss: 0.4302 | train: 0.5788 | Test score: 0.5390\n",
            "Epoch 1256 | loss: 0.4297 | train: 0.5759 | Test score: 0.5446\n",
            "Epoch 1257 | loss: 0.4259 | train: 0.5814 | Test score: 0.5449\n",
            "Epoch 1258 | loss: 0.4247 | train: 0.5739 | Test score: 0.5430\n",
            "Epoch 1259 | loss: 0.4285 | train: 0.5789 | Test score: 0.5470\n",
            "Epoch 1260 | loss: 0.4246 | train: 0.5785 | Test score: 0.5396\n",
            "Epoch 1261 | loss: 0.4285 | train: 0.5783 | Test score: 0.5405\n",
            "Epoch 1262 | loss: 0.4283 | train: 0.5793 | Test score: 0.5378\n",
            "Epoch 1263 | loss: 0.4273 | train: 0.5782 | Test score: 0.5319\n",
            "Epoch 1264 | loss: 0.4318 | train: 0.5817 | Test score: 0.5470\n",
            "Epoch 1265 | loss: 0.4295 | train: 0.5779 | Test score: 0.5449\n",
            "Epoch 1266 | loss: 0.4250 | train: 0.5799 | Test score: 0.5455\n",
            "Epoch 1267 | loss: 0.4249 | train: 0.5791 | Test score: 0.5421\n",
            "Epoch 1268 | loss: 0.4324 | train: 0.5757 | Test score: 0.5520\n",
            "Epoch 1269 | loss: 0.4300 | train: 0.5762 | Test score: 0.5492\n",
            "Epoch 1270 | loss: 0.4227 | train: 0.5747 | Test score: 0.5415\n",
            "Epoch 1271 | loss: 0.4308 | train: 0.5768 | Test score: 0.5384\n",
            "Epoch 1272 | loss: 0.4235 | train: 0.5776 | Test score: 0.5257\n",
            "Epoch 1273 | loss: 0.4285 | train: 0.5777 | Test score: 0.5483\n",
            "Epoch 1274 | loss: 0.4273 | train: 0.5800 | Test score: 0.5384\n",
            "Epoch 1275 | loss: 0.4257 | train: 0.5773 | Test score: 0.5578\n",
            "Epoch 1276 | loss: 0.4273 | train: 0.5755 | Test score: 0.5446\n",
            "Epoch 1277 | loss: 0.4233 | train: 0.5782 | Test score: 0.5458\n",
            "Epoch 1278 | loss: 0.4261 | train: 0.5788 | Test score: 0.5381\n",
            "Epoch 1279 | loss: 0.4279 | train: 0.5757 | Test score: 0.5368\n",
            "Epoch 1280 | loss: 0.4281 | train: 0.5767 | Test score: 0.5504\n",
            "Epoch 1281 | loss: 0.4248 | train: 0.5752 | Test score: 0.5458\n",
            "Epoch 1282 | loss: 0.4267 | train: 0.5762 | Test score: 0.5325\n",
            "Epoch 1283 | loss: 0.4240 | train: 0.5785 | Test score: 0.5412\n",
            "Epoch 1284 | loss: 0.4225 | train: 0.5788 | Test score: 0.5319\n",
            "Epoch 1285 | loss: 0.4253 | train: 0.5791 | Test score: 0.5547\n",
            "Epoch 1286 | loss: 0.4244 | train: 0.5744 | Test score: 0.5479\n",
            "Epoch 1287 | loss: 0.4252 | train: 0.5797 | Test score: 0.5368\n",
            "Epoch 1288 | loss: 0.4299 | train: 0.5755 | Test score: 0.5285\n",
            "Epoch 1289 | loss: 0.4270 | train: 0.5769 | Test score: 0.5486\n",
            "Epoch 1290 | loss: 0.4215 | train: 0.5770 | Test score: 0.5415\n",
            "Epoch 1291 | loss: 0.4257 | train: 0.5751 | Test score: 0.5418\n",
            "Epoch 1292 | loss: 0.4259 | train: 0.5760 | Test score: 0.5384\n",
            "Epoch 1293 | loss: 0.4241 | train: 0.5775 | Test score: 0.5489\n",
            "Epoch 1294 | loss: 0.4249 | train: 0.5767 | Test score: 0.5378\n",
            "Epoch 1295 | loss: 0.4213 | train: 0.5773 | Test score: 0.5402\n",
            "Epoch 1296 | loss: 0.4221 | train: 0.5753 | Test score: 0.5467\n",
            "Epoch 1297 | loss: 0.4226 | train: 0.5757 | Test score: 0.5436\n",
            "Epoch 1298 | loss: 0.4219 | train: 0.5774 | Test score: 0.5338\n",
            "Epoch 1299 | loss: 0.4290 | train: 0.5760 | Test score: 0.5449\n",
            "Epoch 1300 | loss: 0.4264 | train: 0.5741 | Test score: 0.5331\n",
            "Epoch 1301 | loss: 0.4210 | train: 0.5769 | Test score: 0.5285\n",
            "Epoch 1302 | loss: 0.4243 | train: 0.5751 | Test score: 0.5353\n",
            "Epoch 1303 | loss: 0.4218 | train: 0.5774 | Test score: 0.5399\n",
            "Epoch 1304 | loss: 0.4262 | train: 0.5771 | Test score: 0.5319\n",
            "Epoch 1305 | loss: 0.4252 | train: 0.5753 | Test score: 0.5415\n",
            "Epoch 1306 | loss: 0.4233 | train: 0.5751 | Test score: 0.5301\n",
            "Epoch 1307 | loss: 0.4275 | train: 0.5750 | Test score: 0.5375\n",
            "Epoch 1308 | loss: 0.4250 | train: 0.5736 | Test score: 0.5304\n",
            "Epoch 1309 | loss: 0.4301 | train: 0.5740 | Test score: 0.5384\n",
            "Epoch 1310 | loss: 0.4264 | train: 0.5737 | Test score: 0.5282\n",
            "Epoch 1311 | loss: 0.4240 | train: 0.5739 | Test score: 0.5384\n",
            "Epoch 1312 | loss: 0.4260 | train: 0.5789 | Test score: 0.5415\n",
            "Epoch 1313 | loss: 0.4184 | train: 0.5780 | Test score: 0.5402\n",
            "Epoch 1314 | loss: 0.4226 | train: 0.5774 | Test score: 0.5381\n",
            "Epoch 1315 | loss: 0.4256 | train: 0.5753 | Test score: 0.5513\n",
            "Epoch 1316 | loss: 0.4237 | train: 0.5809 | Test score: 0.5365\n",
            "Epoch 1317 | loss: 0.4236 | train: 0.5774 | Test score: 0.5427\n",
            "Epoch 1318 | loss: 0.4238 | train: 0.5762 | Test score: 0.5338\n",
            "Epoch 1319 | loss: 0.4200 | train: 0.5777 | Test score: 0.5486\n",
            "Epoch 1320 | loss: 0.4267 | train: 0.5778 | Test score: 0.5464\n",
            "Epoch 1321 | loss: 0.4283 | train: 0.5767 | Test score: 0.5449\n",
            "Epoch 1322 | loss: 0.4205 | train: 0.5796 | Test score: 0.5479\n",
            "Epoch 1323 | loss: 0.4273 | train: 0.5784 | Test score: 0.5498\n",
            "Epoch 1324 | loss: 0.4254 | train: 0.5727 | Test score: 0.5492\n",
            "Epoch 1325 | loss: 0.4196 | train: 0.5760 | Test score: 0.5526\n",
            "Epoch 1326 | loss: 0.4252 | train: 0.5782 | Test score: 0.5424\n",
            "Epoch 1327 | loss: 0.4199 | train: 0.5796 | Test score: 0.5430\n",
            "Epoch 1328 | loss: 0.4240 | train: 0.5747 | Test score: 0.5405\n",
            "Epoch 1329 | loss: 0.4232 | train: 0.5774 | Test score: 0.5541\n",
            "Epoch 1330 | loss: 0.4206 | train: 0.5756 | Test score: 0.5424\n",
            "Epoch 1331 | loss: 0.4233 | train: 0.5734 | Test score: 0.5470\n",
            "Epoch 1332 | loss: 0.4205 | train: 0.5792 | Test score: 0.5421\n",
            "Epoch 1333 | loss: 0.4188 | train: 0.5726 | Test score: 0.5507\n",
            "Epoch 1334 | loss: 0.4233 | train: 0.5807 | Test score: 0.5483\n",
            "Epoch 1335 | loss: 0.4185 | train: 0.5798 | Test score: 0.5473\n",
            "Epoch 1336 | loss: 0.4285 | train: 0.5753 | Test score: 0.5569\n",
            "Epoch 1337 | loss: 0.4214 | train: 0.5730 | Test score: 0.5415\n",
            "Epoch 1338 | loss: 0.4238 | train: 0.5774 | Test score: 0.5510\n",
            "Epoch 1339 | loss: 0.4178 | train: 0.5750 | Test score: 0.5452\n",
            "Epoch 1340 | loss: 0.4223 | train: 0.5741 | Test score: 0.5442\n",
            "Epoch 1341 | loss: 0.4208 | train: 0.5775 | Test score: 0.5436\n",
            "Epoch 1342 | loss: 0.4199 | train: 0.5777 | Test score: 0.5464\n",
            "Epoch 1343 | loss: 0.4174 | train: 0.5768 | Test score: 0.5458\n",
            "Epoch 1344 | loss: 0.4230 | train: 0.5766 | Test score: 0.5575\n",
            "Epoch 1345 | loss: 0.4239 | train: 0.5740 | Test score: 0.5615\n",
            "Epoch 1346 | loss: 0.4210 | train: 0.5751 | Test score: 0.5368\n",
            "Epoch 1347 | loss: 0.4219 | train: 0.5738 | Test score: 0.5495\n",
            "Epoch 1348 | loss: 0.4243 | train: 0.5756 | Test score: 0.5470\n",
            "Epoch 1349 | loss: 0.4177 | train: 0.5761 | Test score: 0.5356\n",
            "Epoch 1350 | loss: 0.4197 | train: 0.5785 | Test score: 0.5365\n",
            "Epoch 1351 | loss: 0.4240 | train: 0.5780 | Test score: 0.5390\n",
            "Epoch 1352 | loss: 0.4258 | train: 0.5773 | Test score: 0.5430\n",
            "Epoch 1353 | loss: 0.4201 | train: 0.5763 | Test score: 0.5424\n",
            "Epoch 1354 | loss: 0.4201 | train: 0.5757 | Test score: 0.5442\n",
            "Epoch 1355 | loss: 0.4208 | train: 0.5711 | Test score: 0.5424\n",
            "Epoch 1356 | loss: 0.4208 | train: 0.5719 | Test score: 0.5372\n",
            "Epoch 1357 | loss: 0.4174 | train: 0.5762 | Test score: 0.5507\n",
            "Epoch 1358 | loss: 0.4212 | train: 0.5793 | Test score: 0.5356\n",
            "Epoch 1359 | loss: 0.4217 | train: 0.5764 | Test score: 0.5449\n",
            "Epoch 1360 | loss: 0.4207 | train: 0.5763 | Test score: 0.5424\n",
            "Epoch 1361 | loss: 0.4224 | train: 0.5730 | Test score: 0.5372\n",
            "Epoch 1362 | loss: 0.4253 | train: 0.5771 | Test score: 0.5470\n",
            "Epoch 1363 | loss: 0.4186 | train: 0.5732 | Test score: 0.5344\n",
            "Epoch 1364 | loss: 0.4222 | train: 0.5736 | Test score: 0.5375\n",
            "Epoch 1365 | loss: 0.4205 | train: 0.5749 | Test score: 0.5409\n",
            "Epoch 1366 | loss: 0.4170 | train: 0.5755 | Test score: 0.5452\n",
            "Epoch 1367 | loss: 0.4219 | train: 0.5741 | Test score: 0.5402\n",
            "Epoch 1368 | loss: 0.4241 | train: 0.5731 | Test score: 0.5467\n",
            "Epoch 1369 | loss: 0.4188 | train: 0.5745 | Test score: 0.5405\n",
            "Epoch 1370 | loss: 0.4171 | train: 0.5700 | Test score: 0.5421\n",
            "Epoch 1371 | loss: 0.4205 | train: 0.5727 | Test score: 0.5433\n",
            "Epoch 1372 | loss: 0.4169 | train: 0.5741 | Test score: 0.5418\n",
            "Epoch 1373 | loss: 0.4212 | train: 0.5742 | Test score: 0.5544\n",
            "Epoch 1374 | loss: 0.4188 | train: 0.5752 | Test score: 0.5461\n",
            "Epoch 1375 | loss: 0.4225 | train: 0.5725 | Test score: 0.5322\n",
            "Epoch 1376 | loss: 0.4220 | train: 0.5733 | Test score: 0.5313\n",
            "Epoch 1377 | loss: 0.4185 | train: 0.5745 | Test score: 0.5307\n",
            "Epoch 1378 | loss: 0.4211 | train: 0.5736 | Test score: 0.5294\n",
            "Epoch 1379 | loss: 0.4174 | train: 0.5717 | Test score: 0.5322\n",
            "Epoch 1380 | loss: 0.4199 | train: 0.5758 | Test score: 0.5458\n",
            "Epoch 1381 | loss: 0.4178 | train: 0.5747 | Test score: 0.5409\n",
            "Epoch 1382 | loss: 0.4220 | train: 0.5725 | Test score: 0.5399\n",
            "Epoch 1383 | loss: 0.4156 | train: 0.5768 | Test score: 0.5464\n",
            "Epoch 1384 | loss: 0.4143 | train: 0.5752 | Test score: 0.5498\n",
            "Epoch 1385 | loss: 0.4197 | train: 0.5719 | Test score: 0.5455\n",
            "Epoch 1386 | loss: 0.4200 | train: 0.5743 | Test score: 0.5504\n",
            "Epoch 1387 | loss: 0.4181 | train: 0.5774 | Test score: 0.5461\n",
            "Epoch 1388 | loss: 0.4190 | train: 0.5769 | Test score: 0.5433\n",
            "Epoch 1389 | loss: 0.4177 | train: 0.5743 | Test score: 0.5418\n",
            "Epoch 1390 | loss: 0.4193 | train: 0.5776 | Test score: 0.5458\n",
            "Epoch 1391 | loss: 0.4189 | train: 0.5743 | Test score: 0.5347\n",
            "Epoch 1392 | loss: 0.4171 | train: 0.5775 | Test score: 0.5288\n",
            "Epoch 1393 | loss: 0.4167 | train: 0.5753 | Test score: 0.5365\n",
            "Epoch 1394 | loss: 0.4179 | train: 0.5771 | Test score: 0.5442\n",
            "Epoch 1395 | loss: 0.4150 | train: 0.5750 | Test score: 0.5378\n",
            "Epoch 1396 | loss: 0.4204 | train: 0.5762 | Test score: 0.5424\n",
            "Epoch 1397 | loss: 0.4174 | train: 0.5750 | Test score: 0.5458\n",
            "Epoch 1398 | loss: 0.4147 | train: 0.5719 | Test score: 0.5510\n",
            "Epoch 1399 | loss: 0.4153 | train: 0.5731 | Test score: 0.5415\n",
            "Epoch 1400 | loss: 0.4150 | train: 0.5698 | Test score: 0.5328\n",
            "Epoch 1401 | loss: 0.4218 | train: 0.5719 | Test score: 0.5402\n",
            "Epoch 1402 | loss: 0.4185 | train: 0.5744 | Test score: 0.5344\n",
            "Epoch 1403 | loss: 0.4158 | train: 0.5707 | Test score: 0.5446\n",
            "Epoch 1404 | loss: 0.4215 | train: 0.5722 | Test score: 0.5433\n",
            "Epoch 1405 | loss: 0.4166 | train: 0.5729 | Test score: 0.5492\n",
            "Epoch 1406 | loss: 0.4184 | train: 0.5745 | Test score: 0.5322\n",
            "Epoch 1407 | loss: 0.4189 | train: 0.5793 | Test score: 0.5415\n",
            "Epoch 1408 | loss: 0.4185 | train: 0.5719 | Test score: 0.5424\n",
            "Epoch 1409 | loss: 0.4212 | train: 0.5754 | Test score: 0.5569\n",
            "Epoch 1410 | loss: 0.4152 | train: 0.5717 | Test score: 0.5362\n",
            "Epoch 1411 | loss: 0.4192 | train: 0.5751 | Test score: 0.5433\n",
            "Epoch 1412 | loss: 0.4184 | train: 0.5734 | Test score: 0.5473\n",
            "Epoch 1413 | loss: 0.4155 | train: 0.5776 | Test score: 0.5328\n",
            "Epoch 1414 | loss: 0.4192 | train: 0.5742 | Test score: 0.5393\n",
            "Epoch 1415 | loss: 0.4240 | train: 0.5733 | Test score: 0.5461\n",
            "Epoch 1416 | loss: 0.4168 | train: 0.5737 | Test score: 0.5442\n",
            "Epoch 1417 | loss: 0.4185 | train: 0.5746 | Test score: 0.5467\n",
            "Epoch 1418 | loss: 0.4138 | train: 0.5698 | Test score: 0.5442\n",
            "Epoch 1419 | loss: 0.4165 | train: 0.5718 | Test score: 0.5433\n",
            "Epoch 1420 | loss: 0.4166 | train: 0.5744 | Test score: 0.5427\n",
            "Epoch 1421 | loss: 0.4143 | train: 0.5760 | Test score: 0.5516\n",
            "Epoch 1422 | loss: 0.4168 | train: 0.5744 | Test score: 0.5335\n",
            "Epoch 1423 | loss: 0.4148 | train: 0.5750 | Test score: 0.5415\n",
            "Epoch 1424 | loss: 0.4159 | train: 0.5775 | Test score: 0.5362\n",
            "Epoch 1425 | loss: 0.4139 | train: 0.5716 | Test score: 0.5279\n",
            "Epoch 1426 | loss: 0.4142 | train: 0.5763 | Test score: 0.5452\n",
            "Epoch 1427 | loss: 0.4146 | train: 0.5736 | Test score: 0.5495\n",
            "Epoch 1428 | loss: 0.4195 | train: 0.5755 | Test score: 0.5418\n",
            "Epoch 1429 | loss: 0.4151 | train: 0.5735 | Test score: 0.5452\n",
            "Epoch 1430 | loss: 0.4164 | train: 0.5777 | Test score: 0.5433\n",
            "Epoch 1431 | loss: 0.4177 | train: 0.5746 | Test score: 0.5341\n",
            "Epoch 1432 | loss: 0.4159 | train: 0.5701 | Test score: 0.5433\n",
            "Epoch 1433 | loss: 0.4167 | train: 0.5743 | Test score: 0.5433\n",
            "Epoch 1434 | loss: 0.4192 | train: 0.5713 | Test score: 0.5393\n",
            "Epoch 1435 | loss: 0.4155 | train: 0.5729 | Test score: 0.5365\n",
            "Epoch 1436 | loss: 0.4168 | train: 0.5739 | Test score: 0.5430\n",
            "Epoch 1437 | loss: 0.4147 | train: 0.5725 | Test score: 0.5359\n",
            "Epoch 1438 | loss: 0.4164 | train: 0.5714 | Test score: 0.5372\n",
            "Epoch 1439 | loss: 0.4153 | train: 0.5755 | Test score: 0.5217\n",
            "Epoch 1440 | loss: 0.4156 | train: 0.5723 | Test score: 0.5282\n",
            "Epoch 1441 | loss: 0.4132 | train: 0.5741 | Test score: 0.5393\n",
            "Epoch 1442 | loss: 0.4166 | train: 0.5735 | Test score: 0.5495\n",
            "Epoch 1443 | loss: 0.4151 | train: 0.5717 | Test score: 0.5421\n",
            "Epoch 1444 | loss: 0.4154 | train: 0.5771 | Test score: 0.5251\n",
            "Epoch 1445 | loss: 0.4141 | train: 0.5734 | Test score: 0.5436\n",
            "Epoch 1446 | loss: 0.4179 | train: 0.5707 | Test score: 0.5381\n",
            "Epoch 1447 | loss: 0.4153 | train: 0.5733 | Test score: 0.5344\n",
            "Epoch 1448 | loss: 0.4167 | train: 0.5701 | Test score: 0.5409\n",
            "Epoch 1449 | loss: 0.4115 | train: 0.5730 | Test score: 0.5409\n",
            "Epoch 1450 | loss: 0.4144 | train: 0.5710 | Test score: 0.5476\n",
            "Epoch 1451 | loss: 0.4171 | train: 0.5687 | Test score: 0.5430\n",
            "Epoch 1452 | loss: 0.4145 | train: 0.5708 | Test score: 0.5446\n",
            "Epoch 1453 | loss: 0.4122 | train: 0.5740 | Test score: 0.5344\n",
            "Epoch 1454 | loss: 0.4133 | train: 0.5719 | Test score: 0.5381\n",
            "Epoch 1455 | loss: 0.4170 | train: 0.5733 | Test score: 0.5427\n",
            "Epoch 1456 | loss: 0.4109 | train: 0.5715 | Test score: 0.5452\n",
            "Epoch 1457 | loss: 0.4151 | train: 0.5733 | Test score: 0.5464\n",
            "Epoch 1458 | loss: 0.4164 | train: 0.5705 | Test score: 0.5396\n",
            "Epoch 1459 | loss: 0.4174 | train: 0.5713 | Test score: 0.5467\n",
            "Epoch 1460 | loss: 0.4182 | train: 0.5704 | Test score: 0.5529\n",
            "Epoch 1461 | loss: 0.4163 | train: 0.5708 | Test score: 0.5399\n",
            "Epoch 1462 | loss: 0.4138 | train: 0.5735 | Test score: 0.5452\n",
            "Epoch 1463 | loss: 0.4167 | train: 0.5743 | Test score: 0.5489\n",
            "Epoch 1464 | loss: 0.4137 | train: 0.5721 | Test score: 0.5412\n",
            "Epoch 1465 | loss: 0.4111 | train: 0.5702 | Test score: 0.5458\n",
            "Epoch 1466 | loss: 0.4080 | train: 0.5719 | Test score: 0.5498\n",
            "Epoch 1467 | loss: 0.4135 | train: 0.5771 | Test score: 0.5516\n",
            "Epoch 1468 | loss: 0.4133 | train: 0.5724 | Test score: 0.5449\n",
            "Epoch 1469 | loss: 0.4148 | train: 0.5701 | Test score: 0.5449\n",
            "Epoch 1470 | loss: 0.4148 | train: 0.5738 | Test score: 0.5390\n",
            "Epoch 1471 | loss: 0.4141 | train: 0.5726 | Test score: 0.5464\n",
            "Epoch 1472 | loss: 0.4145 | train: 0.5713 | Test score: 0.5399\n",
            "Epoch 1473 | loss: 0.4192 | train: 0.5715 | Test score: 0.5402\n",
            "Epoch 1474 | loss: 0.4079 | train: 0.5740 | Test score: 0.5384\n",
            "Epoch 1475 | loss: 0.4147 | train: 0.5722 | Test score: 0.5492\n",
            "Epoch 1476 | loss: 0.4136 | train: 0.5724 | Test score: 0.5461\n",
            "Epoch 1477 | loss: 0.4082 | train: 0.5677 | Test score: 0.5433\n",
            "Epoch 1478 | loss: 0.4115 | train: 0.5709 | Test score: 0.5495\n",
            "Epoch 1479 | loss: 0.4177 | train: 0.5731 | Test score: 0.5433\n",
            "Epoch 1480 | loss: 0.4154 | train: 0.5744 | Test score: 0.5375\n",
            "Epoch 1481 | loss: 0.4156 | train: 0.5742 | Test score: 0.5523\n",
            "Epoch 1482 | loss: 0.4174 | train: 0.5718 | Test score: 0.5405\n",
            "Epoch 1483 | loss: 0.4119 | train: 0.5747 | Test score: 0.5421\n",
            "Epoch 1484 | loss: 0.4151 | train: 0.5710 | Test score: 0.5368\n",
            "Epoch 1485 | loss: 0.4157 | train: 0.5747 | Test score: 0.5424\n",
            "Epoch 1486 | loss: 0.4118 | train: 0.5708 | Test score: 0.5430\n",
            "Epoch 1487 | loss: 0.4145 | train: 0.5723 | Test score: 0.5464\n",
            "Epoch 1488 | loss: 0.4154 | train: 0.5735 | Test score: 0.5461\n",
            "Epoch 1489 | loss: 0.4148 | train: 0.5718 | Test score: 0.5492\n",
            "Epoch 1490 | loss: 0.4158 | train: 0.5736 | Test score: 0.5452\n",
            "Epoch 1491 | loss: 0.4132 | train: 0.5728 | Test score: 0.5399\n",
            "Epoch 1492 | loss: 0.4158 | train: 0.5740 | Test score: 0.5458\n",
            "Epoch 1493 | loss: 0.4085 | train: 0.5700 | Test score: 0.5523\n",
            "Epoch 1494 | loss: 0.4163 | train: 0.5695 | Test score: 0.5338\n",
            "Epoch 1495 | loss: 0.4126 | train: 0.5728 | Test score: 0.5470\n",
            "Epoch 1496 | loss: 0.4103 | train: 0.5747 | Test score: 0.5409\n",
            "Epoch 1497 | loss: 0.4140 | train: 0.5701 | Test score: 0.5285\n",
            "Epoch 1498 | loss: 0.4137 | train: 0.5707 | Test score: 0.5378\n",
            "Epoch 1499 | loss: 0.4156 | train: 0.5726 | Test score: 0.5418\n",
            "Epoch 1500 | loss: 0.4093 | train: 0.5724 | Test score: 0.5378\n",
            "Epoch 1501 | loss: 0.4098 | train: 0.5696 | Test score: 0.5461\n",
            "Epoch 1502 | loss: 0.4133 | train: 0.5720 | Test score: 0.5455\n",
            "Epoch 1503 | loss: 0.4092 | train: 0.5712 | Test score: 0.5298\n",
            "Epoch 1504 | loss: 0.4111 | train: 0.5690 | Test score: 0.5436\n",
            "Epoch 1505 | loss: 0.4151 | train: 0.5732 | Test score: 0.5464\n",
            "Epoch 1506 | loss: 0.4117 | train: 0.5753 | Test score: 0.5399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"d_main: {hyperparameters['d_min']},d_hidden: {hyperparameters['d_hidden']}, blocks:{hyperparameters['n_blocks']}, Learning_rate: {lr}, weight_decay:{weight_decay}\")\n",
        "epoch_graph=np.arange(1,2001)\n",
        "print(loss_lst[0], loss_lst[-1])\n",
        "plt.plot(range(len(loss_lst)),loss_lst)\n",
        "plt.title('training loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Training loss')\n",
        "plt.show()\n",
        "print(trainacc[0], trainacc[-1])\n",
        "plt.plot(range(len(trainacc)),trainacc)\n",
        "print(testacc[0], testacc[-1])\n",
        "plt.plot(range(len(testacc)),testacc)\n",
        "plt.title('train-test accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IBpUeCt5wLv9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "4c06f7cc-1ce2-4b9e-c178-90e2b88b495e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_main: 32,d_hidden: 128, blocks:2, Learning_rate: 0.0005, weight_decay:0.0005\n",
            "0.73105388879776 0.6010077595710754\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4800795345754474 0.5582517122026659\n",
            "0.5414739438791243 0.5948196114708603\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}