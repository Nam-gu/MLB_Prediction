{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param ={\n",
    "            'num_leaves': [4,5,6,7,8],  \n",
    "             'max_depth':[-1],\n",
    "             'learning_rate': [0.1],\n",
    "             \"n_estimators\": [100],\n",
    "             \"min_child_samples\":[20,40,60,80],\n",
    "             #'subsample': [0.1], \n",
    "             #'colsample_bytree': [1,0.95],\n",
    "             #'reg_alpha': [1,0,1e-1],\n",
    "             #'reg_lambda': [10,5,3,1,0,1e-1,1e-2],\n",
    "            #'random_seed' : [0,1,2,3]\n",
    "            'boosting':['gbdt']\n",
    "             }\n",
    "lgb =LGBMClassifier()\n",
    "lgb_clf = GridSearchCV(lgb,param,scoring='accuracy',n_jobs=num_thread,cv=5)\n",
    "lgb_results = lgb_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "train_score = lgb_results.score(X_train, y_train)\n",
    "test_score = lgb_results.score(X_test, y_test)\n",
    "print(lgb_results.best_params_)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tmp = X_test.columns\n",
    "X_train=X_train.values\n",
    "y_train=y_train.values\n",
    "X_test=X_test.values\n",
    "y_test=y_test.values\n",
    "y_test = np.squeeze(y_test)\n",
    "y_train = np.squeeze(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import ParameterGrid,KFold\n",
    "# baysian search \n",
    "# n_d, n_a, n_steps 적절한 param 찾기\n",
    "test = []\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "preds = []\n",
    "scores  = []\n",
    "param_grid = dict(n_d = [2,4,6,8],\n",
    "                  n_a = [2,4,6,8],\n",
    "                  n_steps = [2, 3, 4, 5],\n",
    "                  #lr = [0.1,0.01,],\n",
    "                  #gamma = [1, 1.5, 2],\n",
    "                  #lambda_sparse = [1e-1, 1e-2, 1e-3],\n",
    "                  #n_shared = [2],\n",
    "                  #n_independent = [2],\n",
    "                  #clip_value = [2.],     \n",
    ")\n",
    "grid = ParameterGrid(param_grid)\n",
    "max_val_acc = 0\n",
    "test_acc =0 \n",
    "best_param = 0\n",
    "for params in grid:\n",
    "    scores = []\n",
    "    preds = []\n",
    "    for trn_idx,val_idx in kf.split(X_train):\n",
    "        X_trn = X_train[trn_idx]\n",
    "        y_trn = y_train[trn_idx]\n",
    "        X_val = X_train[val_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "\n",
    "        clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                            optimizer_params=dict(lr=1e-2),\n",
    "                            scheduler_params={\"step_size\":50,\n",
    "                                                \"gamma\":0.9},\n",
    "                            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                            mask_type='sparsemax' # \"sparsemax\", entmax\n",
    "                            )\n",
    "\n",
    "        max_epochs = 2\n",
    "\n",
    "        clf.fit(\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            eval_set=[(X_trn, y_trn), (X_val, y_val)],\n",
    "            eval_name=['train', 'val'],\n",
    "            eval_metric=['auc','accuracy'],\n",
    "            max_epochs=max_epochs , patience=10,\n",
    "            batch_size=128, virtual_batch_size=64,\n",
    "            num_workers=0,\n",
    "            weights=1,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        scores.append(clf.best_cost)\n",
    "        \n",
    "        pred = clf.predict(X_test)\n",
    "        preds.append(pred)\n",
    "    val_acc = np.mean(scores)\n",
    "    if max_val_acc < val_acc:\n",
    "        max_val_acc = val_acc\n",
    "        best_param = params\n",
    "        test_acc = np.mean(preds)\n",
    "print('best_param: ', best_param)\n",
    "print('best_val_acc: ', max_val_acc)\n",
    "print('test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 n_d, n_a, n_steps best param 넣고 실행\n",
    "param_grid = dict(n_d = [],\n",
    "                  n_a = [],\n",
    "                  n_steps = [],\n",
    "                  lr = [0.1,0.01],\n",
    "                  gamma = [1, 1.5, 2],\n",
    "                  #lambda_sparse = [1e-1, 1e-2, 1e-3],\n",
    "                  n_shared = [1,2,3,4],\n",
    "                  n_independent = [1,2,3,4],  \n",
    ")\n",
    "grid = ParameterGrid(param_grid)\n",
    "max_val_acc = 0\n",
    "test_acc =0 \n",
    "best_param = 0\n",
    "for params in grid:\n",
    "    scores = []\n",
    "    preds = []\n",
    "    for trn_idx,val_idx in kf.split(X_train):\n",
    "        X_trn = X_train[trn_idx]\n",
    "        y_trn = y_train[trn_idx]\n",
    "        X_val = X_train[val_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "\n",
    "        clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                            optimizer_params=dict(lr=1e-2),\n",
    "                            scheduler_params={\"step_size\":50,\n",
    "                                                \"gamma\":0.9},\n",
    "                            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                            mask_type='sparsemax' # \"sparsemax\", entmax\n",
    "                            )\n",
    "\n",
    "        max_epochs = 2\n",
    "\n",
    "        clf.fit(\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            eval_set=[(X_trn, y_trn), (X_val, y_val)],\n",
    "            eval_name=['train', 'val'],\n",
    "            eval_metric=['auc','accuracy'],\n",
    "            max_epochs=max_epochs , patience=10,\n",
    "            batch_size=128, virtual_batch_size=64,\n",
    "            num_workers=0,\n",
    "            weights=1,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        scores.append(clf.best_cost)\n",
    "        \n",
    "        pred = clf.predict(X_test)\n",
    "        preds.append(pred)\n",
    "    val_acc = np.mean(scores)\n",
    "    if max_val_acc < val_acc:\n",
    "        max_val_acc = val_acc\n",
    "        best_param = params\n",
    "        test_acc = np.mean(preds)\n",
    "print('best_param: ', best_param)\n",
    "print('best_val_acc: ', max_val_acc)\n",
    "print('test_acc: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae0d8e3760de6a7a1cf353beebfd97e93272233db579176b0efadca8f2d83c10"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('min': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
